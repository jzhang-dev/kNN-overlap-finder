{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/conda/pkgs')\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "import gzip, json\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "fasta_gz_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/regional_reads/Ecoli/all/ONT/reads.fasta.gz'\n",
    "paf_gz_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/regional_reads/Ecoli/all/ONT/alignment.paf.gz'\n",
    "with gzip.open(paf_gz_file, \"rt\") as file:\n",
    "    max_values = {}  \n",
    "    for row in file:  \n",
    "        columns = row.strip().split('\\t') \n",
    "        query_id = columns[0]  \n",
    "        match_bases = int(columns[9]) \n",
    "        max_values[query_id] = columns \n",
    "        if query_id in max_values:  \n",
    "            if match_bases > int(max_values[query_id][9]):  \n",
    "                max_values[match_bases] = columns\n",
    "        else:  \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle, os, gzip, json, sys, itertools\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from typing import Mapping  \n",
    "import mmh3\n",
    "from itertools import chain  \n",
    "import sharedmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miaocj/docker_dir/kNN-overlap-finder/scripts/../lib\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "from graph import OverlapGraph, GenomicInterval, get_overlap_statistics, remove_false_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "max_n_neighbors = 20\n",
    "MAX_SAMPLE_SIZE = int(1e9)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/feature_matrix.npz\"\n",
    "tsv_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/metadata.tsv.gz\"\n",
    "json_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/read_features.json.gz\"\n",
    "\n",
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12926, 324674, 25276, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_read_intervals(meta_df):\n",
    "    read_intervals = {\n",
    "        i: [GenomicInterval(strand, start, end)]\n",
    "        for i, strand, start, end in zip(\n",
    "            meta_df.index,\n",
    "            meta_df[\"reference_strand\"],\n",
    "            meta_df[\"reference_start\"],\n",
    "            meta_df[\"reference_end\"],\n",
    "        )\n",
    "    }\n",
    "    return read_intervals\n",
    "\n",
    "read_intervals = get_read_intervals(meta_df)\n",
    "\n",
    "reference_graph = OverlapGraph.from_intervals(read_intervals)\n",
    "nr_edges = set((node_1, node_2) for node_1, node_2, data in reference_graph.edges(data=True) if not data['redundant'])\n",
    "connected_component_count = len(list(nx.connected_components(reference_graph)))\n",
    "len(reference_graph.nodes), len(reference_graph.edges), len(nr_edges), connected_component_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1) \n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list,  \n",
    "    *,\n",
    "    seed: int,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,32*repeat))\n",
    "    return new_hash_table\n",
    "def get_simhash(read_features,hash_table):\n",
    "    all_read_simhash = []\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "    return reads_simhash_array\n",
    "hash_table = _get_table(feature_matrix.shape[1],seed = 15232,repeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simhash(read_features,hash_table):\n",
    "    all_read_simhash = []\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "    return reads_simhash_array\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_simhash = get_simhash(read_features,hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reference_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m      3\u001b[0m     graph \u001b[38;5;241m=\u001b[39m OverlapGraph\u001b[38;5;241m.\u001b[39mfrom_neighbor_indices(\n\u001b[1;32m      4\u001b[0m         neighbor_indices\u001b[38;5;241m=\u001b[39mnbr_indices,\n\u001b[1;32m      5\u001b[0m         n_neighbors\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m      6\u001b[0m         read_ids\u001b[38;5;241m=\u001b[39mread_ids,\n\u001b[1;32m      7\u001b[0m         require_mutual_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m     graph_stats \u001b[38;5;241m=\u001b[39m get_overlap_statistics(query_graph\u001b[38;5;241m=\u001b[39mgraph, reference_graph\u001b[38;5;241m=\u001b[39m\u001b[43mreference_graph\u001b[49m)\n\u001b[1;32m     10\u001b[0m     stats \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m: k}\n\u001b[1;32m     11\u001b[0m     stats \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimHash_TF-IDF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m: k, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeat_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_stats}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reference_graph' is not defined"
     ]
    }
   ],
   "source": [
    "df_rows = []\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash_TF-IDF', \"n_neighbors\": k, \"repeat_time\": 100,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df_tfidf = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_concat_simhash = get_simhash(read_features,feature_matrix,hash_table,tf = None,idf = None,processes=8) \n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(mp_concat_simhash)  \n",
    "indices = nbrs.kneighbors(mp_concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n",
    "\n",
    "df_rows = []\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash_TF-IDF', \"n_neighbors\": k, \"repeat_time\": 100,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df_tfidf = pd.DataFrame(df_rows)\n",
    "df_tfidf.to_csv('/home/miaocj/docker_dir/kNN-overlap-finder/data/evaluation/human/HLA/ONT_R9/kmer_k16/SimHash_None_None_overlap_stat.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_simhash_array = get_simhash(ref_read_features,hash_table)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(ref_reads_simhash_array)\n",
    "indices = nbrs.kneighbors(que_reads_simhash_array,return_distance=False)\n",
    "print(\"done\\nevaluates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash = feature_matrix@hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_table(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,3200))\n",
    "    return new_hash_table\n",
    "\n",
    "def _get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    all_read_simhash = []\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        weighted_hash_table = hash_table*idf[:, np.newaxis]\n",
    "    else:\n",
    "        weighted_hash_table = hash_table\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(weighted_hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "\n",
    "    return reads_simhash_array \n",
    "\n",
    "hash_table = _get_table(read_features,feature_matrix,seed = 15232,tf = False,idf=False,repeat=100)\n",
    "print(\"1\")\n",
    "reads_simhash_array = _get_simhash(read_features,feature_matrix,hash_table,seed = 15232,tf = True,idf=False,repeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty((kmer_num,32), dtype=np.int8) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index,:]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i,:] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "hash_table = mp_get_hashtable(read_features,feature_matrix,15232,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_read_hash = np.sum(hash_table[list(read_features.values())[0],:,:],axis=0)\n",
    "hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table=np.reshape(hash_table,(kmer_num,3200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_hash[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty(kmer_num, dtype=object) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "## multi-process calculate simhash value\n",
    "def mp_get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    processes:int,):\n",
    "    \n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()} \n",
    "\n",
    "       \n",
    "    reads_simhash_array = np.empty((len(read_features),hash_table.shape[1]*32),dtype=object)\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            read_kmer = list(read_features.values())[i]\n",
    "            one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "            conc_hash = np.concatenate(one_read_hash)\n",
    "            result = np.where(conc_hash > 0, 1, 0)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            reads_simhash_array[i,:] = result            \n",
    "        pool.map(work, range(len(read_features)), reduce=reduce)\n",
    "    return reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table = mp_get_hashtable(feature_matrix,repeat =100, seed = 4829,processes=12)\n",
    "print('hashtable established')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3\n",
    "\n",
    "for repeat in [100]:\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829,tf = True,idf = True)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 100\n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "all_read_simhash = []\n",
    "rng = np.random.default_rng(4829)  \n",
    "hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)\n",
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "for flag,seed in enumerate(hash_seeds):\n",
    "    print(flag)\n",
    "    kmer_hash_indice = {} \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "new_hash = hash_table*idf[:, np.newaxis]\n",
    "\n",
    "for read_ind,read_kmer in read_features.items():\n",
    "    one_read_hash = np.sum(new_hash[read_kmer,:],axis=0)\n",
    "    conc_hash = np.concatenate(one_read_hash)\n",
    "    simhash = np.where(conc_hash > 0, 1, 0)\n",
    "    all_read_simhash.append(simhash)\n",
    "reads_simhash_array = np.array(all_read_simhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(reads_simhash_array)  \n",
    "indices = nbrs.kneighbors(reads_simhash_array,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n",
    "\n",
    "read_ids = np.array(list(read_features))\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##带tf idf 初始版本\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    all_read_simhash = []\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:set(v) for k,v in read_features}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        conc_hash = np.concatenate(one_read_hash)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "  \n",
    "    return reads_simhash_array \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##32位，70个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "\n",
    "    hash_seeds = np.array(range(repeat))\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    kmer_index = np.array(range(kmer_num))\n",
    "\n",
    "    table = []\n",
    "    for seed in hash_seeds:\n",
    "        prime1 = 2654435761  # A large prime number\n",
    "        prime2 = 0x27d4eb2d  # Another large prime, often used in hashing\n",
    "        hash_value = (kmer_index * prime1) ^ (seed * prime2)\n",
    "        hash_t = hash_value % (2**32)\n",
    "        binary_matrix = np.vectorize(np.binary_repr)(hash_t, width=32)\n",
    "        one_repear_table = np.array([list(row) for row in binary_matrix.flatten()])\n",
    "        table.append(one_repear_table.astype(int))\n",
    "    hash_table = np.hstack(table)\n",
    "\n",
    "    all_read_simhash = []\n",
    "    for read_ind,read_kmer in read_features.items():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    concat_simhash = np.array(all_read_simhash)\n",
    "    return concat_simhash \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "dfs = []\n",
    "for repeat in range(20,110,10):\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graphs[k] = graph\n",
    "\n",
    "    df_rows = []\n",
    "    for k in k_values:\n",
    "        graph = graphs[k]\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    dfs.append(df)\n",
    "    \n",
    "new = pd.concat(dfs)\n",
    "new.to_csv('/home/miaocj/docker_dir/test_simhash.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "feature_weights = {i: 1 for i in range(feature_matrix.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))\n",
    "graphs = collections.defaultdict(dict)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graphs[k] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = graphs[k]\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \n",
    "                 **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##70\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##40\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##20\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试hash方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def simhash(features):\n",
    "  \n",
    "  # Generate a hash for each feature\n",
    "  hashes = [hashlib.sha1(feature).hexdigest() for feature in features]\n",
    "  \n",
    "  # Combine the feature hashes to produce the final simhash\n",
    "  concatenated_hash = ''.join(hashes)\n",
    "  simhash = hashlib.sha1(concatenated_hash).hexdigest()\n",
    "  \n",
    "  return simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "x = np.random.rand(1024 * 1024 * 16)\n",
    "h = xxhash.xxh64()\n",
    "h.update(x); h.intdigest(); h.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 64位， 40个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:064b}\".format(hash_value & 0xFFFFFFFFFFFFFFFF)\n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]):  \n",
    "            kmer_index_str = str(kmer_index)  \n",
    "            hash_list = _hash(kmer_index_str, seed=s)  \n",
    "            kmer_hash_indice[kmer_index] = hash_list  \n",
    "  \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 40, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed)\n",
    "beta = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "x = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u1 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u2 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "\n",
    "for j_sample in range(0, sample_count):\n",
    "    feature_indices = sparse.find(data[:, j_sample] > 0)[0]\n",
    "    gamma = -np.log(np.multiply(u1[feature_indices, :], u2[feature_indices, :]))\n",
    "    t_matrix = np.floor(\n",
    "        np.divide(\n",
    "            matlib.repmat(\n",
    "                np.log(data[feature_indices, j_sample].todense()),\n",
    "                1,\n",
    "                dimension_count,\n",
    "            ),\n",
    "            gamma,\n",
    "        )\n",
    "        + beta[feature_indices, :]\n",
    "    )\n",
    "    y_matrix = np.exp(np.multiply(gamma, t_matrix - beta[feature_indices, :]))\n",
    "    a_matrix = np.divide(\n",
    "        -np.log(x[feature_indices, :]),\n",
    "        np.divide(y_matrix, u1[feature_indices, :]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##建立hash矩阵\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    hash_table = np.empty((repeat,feature_matrix.shape[1]))  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]): \n",
    "            hash_table[s,kmer_index] = _hash(kmer_index, seed=s)\n",
    "    for \n",
    "         \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 70, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
