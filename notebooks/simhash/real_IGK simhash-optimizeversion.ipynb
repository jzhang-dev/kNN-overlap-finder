{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/conda/pkgs')\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "import gzip, json\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle, os, gzip, json, sys, itertools\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from typing import Mapping  \n",
    "import mmh3\n",
    "from itertools import chain  \n",
    "import sharedmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "from graph import OverlapGraph, GenomicInterval, get_overlap_statistics, remove_false_edges\n",
    "from nearest_neighbors import HNSW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "import random\n",
    "max_n_neighbors = 20\n",
    "MAX_SAMPLE_SIZE = int(1e9)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "random.seed(12)  # 这里的10可以替换为你希望的种子值  \n",
    "random_number = secrets.token_bytes(128)\n",
    "random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "binary_array = format(random_integer, '01024b')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/chr22/ONT_R9/kmer_k16/feature_matrix.npz\"\n",
    "tsv_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/chr22/ONT_R9/kmer_k16/metadata.tsv.gz\"\n",
    "json_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/chr22/ONT_R9/kmer_k16/read_features.json.gz\"\n",
    "\n",
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139853"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法7\n",
    "import xxhash  \n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    x = xxhash.xxh32(seed=20141025)\n",
    "    x.update(str(kmer_index))\n",
    "    seed = x.intdigest()\n",
    "    rng = np.random.default_rng(seed)  \n",
    "    binary_array = rng.integers(0, 2, size=3200)\n",
    "    return binary_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8)\n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法一：用secret函数 kmer_index 直接作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    random.seed(kmer_index)\n",
    "    random_number = secrets.token_bytes(400)\n",
    "    random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "    binary_array = format(random_integer, '03200b')\n",
    "    hash_array = list(binary_array)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8)\n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法二：用secret函数 kmer_index的哈希值作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = mmh3.hash(str(kmer_index))\n",
    "    random.seed(hash_value)\n",
    "    random_number = secrets.token_bytes(400)\n",
    "    random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "    binary_array = format(random_integer, '03200b')\n",
    "    hash_array = list(binary_array)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index) \n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     hash_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(hash_table \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hash_table\n\u001b[0;32m---> 21\u001b[0m hash_table \u001b[38;5;241m=\u001b[39m \u001b[43m_get_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m, in \u001b[0;36m_get_table\u001b[0;34m(kmer_num)\u001b[0m\n\u001b[1;32m     15\u001b[0m hash_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((kmer_num,\u001b[38;5;241m3200\u001b[39m),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kmer_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kmer_num):\n\u001b[0;32m---> 17\u001b[0m     hash_array \u001b[38;5;241m=\u001b[39m \u001b[43m_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkmer_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     hash_table[kmer_index,:]\u001b[38;5;241m=\u001b[39mhash_array\n\u001b[1;32m     19\u001b[0m hash_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(hash_table \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36m_hash\u001b[0;34m(kmer_index)\u001b[0m\n\u001b[1;32m      6\u001b[0m binary_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[0;32m----> 8\u001b[0m     rng \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m     random2 \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n\u001b[1;32m     10\u001b[0m     bin_arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mbin\u001b[39m(random2)[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m32\u001b[39m))\n",
      "File \u001b[0;32mnumpy/random/_generator.pyx:4957\u001b[0m, in \u001b[0;36mnumpy.random._generator.default_rng\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pcg64.pyx:132\u001b[0m, in \u001b[0;36mnumpy.random._pcg64.PCG64.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##法5：多次生成随机数 时间太久了\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = mmh3.hash(str(kmer_index))\n",
    "    random.seed(hash_value)\n",
    "    seeds = [random.randint(0,2**32 - 1) for _ in range(100)]\n",
    "    binary_array = []\n",
    "    for seed in seeds:\n",
    "        rng = np.random.default_rng(seed) \n",
    "        random2 = rng.integers(0, 2**32-1, dtype=np.uint32)\n",
    "        bin_arrays = list(bin(random2)[2:].zfill(32))\n",
    "        binary_array += bin_arrays\n",
    "    return binary_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8)\n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法6：生成100个随机数再合并\n",
    "import xxhash\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    x = xxhash.xxh32(seed=20141025)\n",
    "    x.update(str(kmer_index))\n",
    "    seed = x.intdigest()\n",
    "    rng = np.random.default_rng(seed) \n",
    "    randoms2 = rng.integers(0, 2**32-1,size=100,dtype=np.uint32)\n",
    "    binary_array = []\n",
    "    for random2 in randoms2:\n",
    "        bin_arrays = list(bin(random2)[2:].zfill(32))\n",
    "        binary_array += bin_arrays\n",
    "    return binary_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8)\n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table2 = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/chr22/ONT_R9/kmer_k16/reference_graph.pkl', 'rb') as file:  \n",
    "    reference_graph = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_value = mmh3.hash(str(562))\n",
    "random.seed(hash_value)\n",
    "seeds = [random.randint(0,2**32 - 1) for _ in range(100)]\n",
    "binary_array = []\n",
    "for seed in seeds:\n",
    "    rng = np.random.default_rng(seed) \n",
    "    random2 = rng.integers(0, 2**32-1, dtype=np.uint32)\n",
    "    bin_arrays = list(bin(random2)[2:].zfill(32))\n",
    "    binary_array += bin_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法三：用random模块 哈希值作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = mmh3.hash(str(kmer_index))\n",
    "    random.seed(hash_value)\n",
    "    hash_array = list(bin(random.getrandbits(3200))[2:].zfill(3200))\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = xxhash.xxh128(str(kmer_index)).intdigest()\n",
    "    rng = np.random.default_rng(hash_value)  \n",
    "    hash_array = rng.integers(0, 2, size=3200)   \n",
    "    return hash_array \n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(30)  \n",
    "binary_array = rng.integers(0, 2, size=3200)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list,  \n",
    "    *,\n",
    "    seed: int,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,32*repeat))\n",
    "    return new_hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.load('/home/miaocj/docker_dir/data/metagenome/hash_table.npz')\n",
    "hash_table2 = new['arr_0'][:feature_matrix.shape[1],:]\n",
    "hash_table3 = hash_table2.astype(np.int8)\n",
    "hash_table4 = np.where(hash_table3 == 0, -1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15055916, 1920)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new['arr_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_simhash(\n",
    "    read_features: list,  \n",
    "    hash_table) -> Mapping[int,list]:  \n",
    "    all_read_simhash = []\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "    return reads_simhash_array\n",
    "reads_simhash_array = _get_simhash(read_features,hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash = feature_matrix@hash_table\n",
    "reads_simhash_array = np.where(simhash > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hamming求解方式二 vptree\n",
    "\n",
    "import pynear\n",
    "vptree = pynear.VPTreeBinaryIndex()\n",
    "vptree.set(reads_simhash_array.astype(np.uint8))\n",
    "vptree_indices, vptree_distances = vptree.searchKNN(reads_simhash_array.astype(np.uint8), 21)\n",
    "vp_nbr= np.array(vptree_indices)[:,:-1][:,::-1]\n",
    "\n",
    "df_rows = []\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=vp_nbr,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids, \n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'method3', \"n_neighbors\": k, \"repeat_time\": 1,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法一 ：哈希长度为1028\n",
    "df\n",
    "df.to_csv('/home/miaocj/docker_dir/kNN-overlap-finder/data/evaluation/human/HLA/ONT_R9/kmer_k16/SimHash_method3_overlap_stat.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_simhash_array = get_simhash(ref_read_features,hash_table)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(ref_reads_simhash_array)\n",
    "indices = nbrs.kneighbors(que_reads_simhash_array,return_distance=False)\n",
    "print(\"done\\nevaluates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_table(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,3200))\n",
    "    return new_hash_table\n",
    "\n",
    "def _get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    all_read_simhash = []\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        weighted_hash_table = hash_table*idf[:, np.newaxis]\n",
    "    else:\n",
    "        weighted_hash_table = hash_table\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(weighted_hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "\n",
    "    return reads_simhash_array \n",
    "\n",
    "hash_table = _get_table(read_features,feature_matrix,seed = 15232,tf = False,idf=False,repeat=100)\n",
    "print(\"1\")\n",
    "reads_simhash_array = _get_simhash(read_features,feature_matrix,hash_table,seed = 15232,tf = True,idf=False,repeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty((kmer_num,32), dtype=np.int8) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index,:]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i,:] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "hash_table = mp_get_hashtable(read_features,feature_matrix,15232,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_read_hash = np.sum(hash_table[list(read_features.values())[0],:,:],axis=0)\n",
    "hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table=np.reshape(hash_table,(kmer_num,3200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_hash[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_read_intervals(meta_df):\n",
    "    read_intervals = {\n",
    "        i: [GenomicInterval(strand, start, end)]\n",
    "        for i, strand, start, end in zip(\n",
    "            meta_df.index,\n",
    "            meta_df[\"reference_strand\"],\n",
    "            meta_df[\"reference_start\"],\n",
    "            meta_df[\"reference_end\"],\n",
    "        )\n",
    "    }\n",
    "    return read_intervals\n",
    "\n",
    "read_intervals = get_read_intervals(meta_df)\n",
    "\n",
    "reference_graph = OverlapGraph.from_intervals(read_intervals)\n",
    "nr_edges = set((node_1, node_2) for node_1, node_2, data in reference_graph.edges(data=True) if not data['redundant'])\n",
    "connected_component_count = len(list(nx.connected_components(reference_graph)))\n",
    "len(reference_graph.nodes), len(reference_graph.edges), len(nr_edges), connected_component_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty(kmer_num, dtype=object) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "## multi-process calculate simhash value\n",
    "def mp_get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    processes:int,):\n",
    "    \n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()} \n",
    "\n",
    "       \n",
    "    reads_simhash_array = np.empty((len(read_features),hash_table.shape[1]*32),dtype=object)\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            read_kmer = list(read_features.values())[i]\n",
    "            one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "            conc_hash = np.concatenate(one_read_hash)\n",
    "            result = np.where(conc_hash > 0, 1, 0)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            reads_simhash_array[i,:] = result            \n",
    "        pool.map(work, range(len(read_features)), reduce=reduce)\n",
    "    return reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table = mp_get_hashtable(feature_matrix,repeat =100, seed = 4829,processes=12)\n",
    "print('hashtable established')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3\n",
    "\n",
    "for repeat in [100]:\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829,tf = True,idf = True)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 100\n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "all_read_simhash = []\n",
    "rng = np.random.default_rng(4829)  \n",
    "hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)\n",
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "for flag,seed in enumerate(hash_seeds):\n",
    "    print(flag)\n",
    "    kmer_hash_indice = {} \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "new_hash = hash_table*idf[:, np.newaxis]\n",
    "\n",
    "for read_ind,read_kmer in read_features.items():\n",
    "    one_read_hash = np.sum(new_hash[read_kmer,:],axis=0)\n",
    "    conc_hash = np.concatenate(one_read_hash)\n",
    "    simhash = np.where(conc_hash > 0, 1, 0)\n",
    "    all_read_simhash.append(simhash)\n",
    "reads_simhash_array = np.array(all_read_simhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(reads_simhash_array)  \n",
    "indices = nbrs.kneighbors(reads_simhash_array,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n",
    "\n",
    "read_ids = np.array(list(read_features))\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##带tf idf 初始版本\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    all_read_simhash = []\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:set(v) for k,v in read_features}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        conc_hash = np.concatenate(one_read_hash)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "  \n",
    "    return reads_simhash_array \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##32位，70个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "\n",
    "    hash_seeds = np.array(range(repeat))\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    kmer_index = np.array(range(kmer_num))\n",
    "\n",
    "    table = []\n",
    "    for seed in hash_seeds:\n",
    "        prime1 = 2654435761  # A large prime number\n",
    "        prime2 = 0x27d4eb2d  # Another large prime, often used in hashing\n",
    "        hash_value = (kmer_index * prime1) ^ (seed * prime2)\n",
    "        hash_t = hash_value % (2**32)\n",
    "        binary_matrix = np.vectorize(np.binary_repr)(hash_t, width=32)\n",
    "        one_repear_table = np.array([list(row) for row in binary_matrix.flatten()])\n",
    "        table.append(one_repear_table.astype(int))\n",
    "    hash_table = np.hstack(table)\n",
    "\n",
    "    all_read_simhash = []\n",
    "    for read_ind,read_kmer in read_features.items():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    concat_simhash = np.array(all_read_simhash)\n",
    "    return concat_simhash \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "dfs = []\n",
    "for repeat in range(20,110,10):\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graphs[k] = graph\n",
    "\n",
    "    df_rows = []\n",
    "    for k in k_values:\n",
    "        graph = graphs[k]\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    dfs.append(df)\n",
    "    \n",
    "new = pd.concat(dfs)\n",
    "new.to_csv('/home/miaocj/docker_dir/test_simhash.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "feature_weights = {i: 1 for i in range(feature_matrix.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))\n",
    "graphs = collections.defaultdict(dict)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graphs[k] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = graphs[k]\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \n",
    "                 **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##70\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##40\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##20\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试hash方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def simhash(features):\n",
    "  \n",
    "  # Generate a hash for each feature\n",
    "  hashes = [hashlib.sha1(feature).hexdigest() for feature in features]\n",
    "  \n",
    "  # Combine the feature hashes to produce the final simhash\n",
    "  concatenated_hash = ''.join(hashes)\n",
    "  simhash = hashlib.sha1(concatenated_hash).hexdigest()\n",
    "  \n",
    "  return simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "x = np.random.rand(1024 * 1024 * 16)\n",
    "h = xxhash.xxh64()\n",
    "h.update(x); h.intdigest(); h.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 64位， 40个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:064b}\".format(hash_value & 0xFFFFFFFFFFFFFFFF)\n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]):  \n",
    "            kmer_index_str = str(kmer_index)  \n",
    "            hash_list = _hash(kmer_index_str, seed=s)  \n",
    "            kmer_hash_indice[kmer_index] = hash_list  \n",
    "  \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 40, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed)\n",
    "beta = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "x = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u1 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u2 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "\n",
    "for j_sample in range(0, sample_count):\n",
    "    feature_indices = sparse.find(data[:, j_sample] > 0)[0]\n",
    "    gamma = -np.log(np.multiply(u1[feature_indices, :], u2[feature_indices, :]))\n",
    "    t_matrix = np.floor(\n",
    "        np.divide(\n",
    "            matlib.repmat(\n",
    "                np.log(data[feature_indices, j_sample].todense()),\n",
    "                1,\n",
    "                dimension_count,\n",
    "            ),\n",
    "            gamma,\n",
    "        )\n",
    "        + beta[feature_indices, :]\n",
    "    )\n",
    "    y_matrix = np.exp(np.multiply(gamma, t_matrix - beta[feature_indices, :]))\n",
    "    a_matrix = np.divide(\n",
    "        -np.log(x[feature_indices, :]),\n",
    "        np.divide(y_matrix, u1[feature_indices, :]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##建立hash矩阵\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    hash_table = np.empty((repeat,feature_matrix.shape[1]))  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]): \n",
    "            hash_table[s,kmer_index] = _hash(kmer_index, seed=s)\n",
    "    for \n",
    "         \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 70, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
