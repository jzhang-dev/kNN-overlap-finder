{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/conda/pkgs')\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "import gzip, json\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "fasta_gz_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/regional_reads/Ecoli/all/ONT/reads.fasta.gz'\n",
    "paf_gz_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/regional_reads/Ecoli/all/ONT/alignment.paf.gz'\n",
    "with gzip.open(paf_gz_file, \"rt\") as file:\n",
    "    max_values = {}  \n",
    "    for row in file:  \n",
    "        columns = row.strip().split('\\t') \n",
    "        query_id = columns[0]  \n",
    "        match_bases = int(columns[9]) \n",
    "        max_values[query_id] = columns \n",
    "        if query_id in max_values:  \n",
    "            if match_bases > int(max_values[query_id][9]):  \n",
    "                max_values[match_bases] = columns\n",
    "        else:  \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle, os, gzip, json, sys, itertools\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from typing import Mapping  \n",
    "import mmh3\n",
    "from itertools import chain  \n",
    "import sharedmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miaocj/docker_dir/kNN-overlap-finder/scripts/../lib\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "from graph import OverlapGraph, GenomicInterval, get_overlap_statistics, remove_false_edges\n",
    "from nearest_neighbors import HNSW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "import random\n",
    "max_n_neighbors = 20\n",
    "MAX_SAMPLE_SIZE = int(1e9)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "random.seed(12)  # 这里的10可以替换为你希望的种子值  \n",
    "random_number = secrets.token_bytes(128)\n",
    "random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "binary_array = format(random_integer, '01024b')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/feature_matrix.npz\"\n",
    "tsv_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/metadata.tsv.gz\"\n",
    "json_path = \"/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/human/HLA/ONT_R9/kmer_k16/read_features.json.gz\"\n",
    "\n",
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12926, 324674, 25276, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_read_intervals(meta_df):\n",
    "    read_intervals = {\n",
    "        i: [GenomicInterval(strand, start, end)]\n",
    "        for i, strand, start, end in zip(\n",
    "            meta_df.index,\n",
    "            meta_df[\"reference_strand\"],\n",
    "            meta_df[\"reference_start\"],\n",
    "            meta_df[\"reference_end\"],\n",
    "        )\n",
    "    }\n",
    "    return read_intervals\n",
    "\n",
    "read_intervals = get_read_intervals(meta_df)\n",
    "\n",
    "reference_graph = OverlapGraph.from_intervals(read_intervals)\n",
    "nr_edges = set((node_1, node_2) for node_1, node_2, data in reference_graph.edges(data=True) if not data['redundant'])\n",
    "connected_component_count = len(list(nx.connected_components(reference_graph)))\n",
    "len(reference_graph.nodes), len(reference_graph.edges), len(nr_edges), connected_component_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法一：用secret函数 kmer_index 直接作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    random.seed(kmer_index)\n",
    "    random_number = secrets.token_bytes(400)\n",
    "    random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "    binary_array = format(random_integer, '03200b')\n",
    "    hash_array = list(binary_array)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法二：用secret函数 kmer_index的哈希值作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = mmh3.hash(str(kmer_index))\n",
    "    random.seed(hash_value)\n",
    "    random_number = secrets.token_bytes(400)\n",
    "    random_integer = int.from_bytes(random_number, byteorder='big')  \n",
    "    binary_array = format(random_integer, '03200b')\n",
    "    hash_array = list(binary_array)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index) \n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法三：用random模块 哈希值作为种子\n",
    "def _hash(kmer_index: int) -> np.ndarray:\n",
    "    hash_value = mmh3.hash(str(kmer_index))\n",
    "    random.seed(hash_value)\n",
    "    hash_array = list(bin(random.getrandbits(3200))[2:].zfill(3200))\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list) -> Mapping[int,list]:\n",
    "    hash_table = np.empty((kmer_num,3200),dtype=np.int8) \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_array = _hash(kmer_index)\n",
    "        hash_table[kmer_index,:]=hash_array\n",
    "    hash_table = np.where(hash_table == 0, -1, 1) \n",
    "    return hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)\n",
    "    return hash_array\n",
    "def _get_table(\n",
    "    kmer_num: list,  \n",
    "    *,\n",
    "    seed: int,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,32*repeat))\n",
    "    return new_hash_table\n",
    "hash_table = _get_table(feature_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.load('/home/miaocj/docker_dir/data/metagenome/hash_table.npz')\n",
    "hash_table2 = new['arr_0'][:feature_matrix.shape[1],:]\n",
    "hash_table3 = hash_table2.astype(np.int8)\n",
    "hash_table4 = np.where(hash_table3 == 0, -1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15055916, 1920)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new['arr_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_simhash(\n",
    "    read_features: list,  \n",
    "    hash_table) -> Mapping[int,list]:  \n",
    "    all_read_simhash = []\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "    return reads_simhash_array\n",
    "reads_simhash_array = _get_simhash(read_features,hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash = feature_matrix@hash_table\n",
    "reads_simhash_array = np.where(simhash > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(reads_simhash_array)  \n",
    "indices = nbrs.kneighbors(reads_simhash_array,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n",
    "\n",
    "df_rows = []\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'method3', \"n_neighbors\": k, \"repeat_time\": 1,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)\n",
    "#df_tfidf.to_csv('/home/miaocj/docker_dir/kNN-overlap-finder/data/evaluation/human/HLA/ONT_R9/kmer_k16/SimHash_None_None_overlap_stat.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##法一 ：哈希长度为1028\n",
    "df\n",
    "df.to_csv('/home/miaocj/docker_dir/kNN-overlap-finder/data/evaluation/human/HLA/ONT_R9/kmer_k16/SimHash_method3_overlap_stat.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>repeat_time</th>\n",
       "      <th>precision</th>\n",
       "      <th>nr_precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>nr_recall</th>\n",
       "      <th>singleton_count</th>\n",
       "      <th>singleton_fraction</th>\n",
       "      <th>N50</th>\n",
       "      <th>component_sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>method3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991711</td>\n",
       "      <td>0.325411</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.257834</td>\n",
       "      <td>120</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>176</td>\n",
       "      <td>[648, 570, 444, 404, 364, 338, 330, 313, 295, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>method3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.288545</td>\n",
       "      <td>0.089588</td>\n",
       "      <td>0.335338</td>\n",
       "      <td>83</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>2250</td>\n",
       "      <td>[2869, 2481, 2250, 1288, 1203, 600, 599, 486, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>method3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>0.263292</td>\n",
       "      <td>0.117490</td>\n",
       "      <td>0.401844</td>\n",
       "      <td>63</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>3956</td>\n",
       "      <td>[6450, 3956, 2494, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>method3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987573</td>\n",
       "      <td>0.242501</td>\n",
       "      <td>0.144899</td>\n",
       "      <td>0.457034</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>6450</td>\n",
       "      <td>[6452, 6450, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>method3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986382</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.172009</td>\n",
       "      <td>0.503521</td>\n",
       "      <td>47</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>6451</td>\n",
       "      <td>[6453, 6451, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>method3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985119</td>\n",
       "      <td>0.211648</td>\n",
       "      <td>0.198596</td>\n",
       "      <td>0.548069</td>\n",
       "      <td>44</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>6452</td>\n",
       "      <td>[6453, 6452, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>method3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984119</td>\n",
       "      <td>0.199582</td>\n",
       "      <td>0.224832</td>\n",
       "      <td>0.585694</td>\n",
       "      <td>42</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>6452</td>\n",
       "      <td>[6453, 6452, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>method3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982969</td>\n",
       "      <td>0.190071</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>0.623002</td>\n",
       "      <td>39</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>method3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981669</td>\n",
       "      <td>0.181148</td>\n",
       "      <td>0.276271</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>38</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>method3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980382</td>\n",
       "      <td>0.173465</td>\n",
       "      <td>0.301533</td>\n",
       "      <td>0.685314</td>\n",
       "      <td>37</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>method3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979113</td>\n",
       "      <td>0.166656</td>\n",
       "      <td>0.326293</td>\n",
       "      <td>0.713404</td>\n",
       "      <td>36</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>method3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.977571</td>\n",
       "      <td>0.160377</td>\n",
       "      <td>0.351038</td>\n",
       "      <td>0.739753</td>\n",
       "      <td>33</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>method3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975981</td>\n",
       "      <td>0.154892</td>\n",
       "      <td>0.375201</td>\n",
       "      <td>0.764876</td>\n",
       "      <td>33</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>method3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974043</td>\n",
       "      <td>0.149784</td>\n",
       "      <td>0.398742</td>\n",
       "      <td>0.787625</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>6453</td>\n",
       "      <td>[6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>method3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971868</td>\n",
       "      <td>0.144932</td>\n",
       "      <td>0.421786</td>\n",
       "      <td>0.807960</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>6454</td>\n",
       "      <td>[6457, 6454, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>method3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969509</td>\n",
       "      <td>0.140435</td>\n",
       "      <td>0.444529</td>\n",
       "      <td>0.827109</td>\n",
       "      <td>29</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>6455</td>\n",
       "      <td>[6457, 6455, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>method3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966888</td>\n",
       "      <td>0.136162</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.844042</td>\n",
       "      <td>26</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>6456</td>\n",
       "      <td>[6458, 6456, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>method3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963898</td>\n",
       "      <td>0.132026</td>\n",
       "      <td>0.488385</td>\n",
       "      <td>0.859274</td>\n",
       "      <td>26</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>6456</td>\n",
       "      <td>[6458, 6456, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>method3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.960505</td>\n",
       "      <td>0.128585</td>\n",
       "      <td>0.509425</td>\n",
       "      <td>0.876009</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>6458</td>\n",
       "      <td>[6459, 6458, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description  n_neighbors  repeat_time  precision  nr_precision    recall  \\\n",
       "0      method3            2            1   0.991711      0.325411  0.061172   \n",
       "1      method3            3            1   0.990196      0.288545  0.089588   \n",
       "2      method3            4            1   0.988828      0.263292  0.117490   \n",
       "3      method3            5            1   0.987573      0.242501  0.144899   \n",
       "4      method3            6            1   0.986382      0.224787  0.172009   \n",
       "5      method3            7            1   0.985119      0.211648  0.198596   \n",
       "6      method3            8            1   0.984119      0.199582  0.224832   \n",
       "7      method3            9            1   0.982969      0.190071  0.250827   \n",
       "8      method3           10            1   0.981669      0.181148  0.276271   \n",
       "9      method3           11            1   0.980382      0.173465  0.301533   \n",
       "10     method3           12            1   0.979113      0.166656  0.326293   \n",
       "11     method3           13            1   0.977571      0.160377  0.351038   \n",
       "12     method3           14            1   0.975981      0.154892  0.375201   \n",
       "13     method3           15            1   0.974043      0.149784  0.398742   \n",
       "14     method3           16            1   0.971868      0.144932  0.421786   \n",
       "15     method3           17            1   0.969509      0.140435  0.444529   \n",
       "16     method3           18            1   0.966888      0.136162  0.466600   \n",
       "17     method3           19            1   0.963898      0.132026  0.488385   \n",
       "18     method3           20            1   0.960505      0.128585  0.509425   \n",
       "\n",
       "    nr_recall  singleton_count  singleton_fraction   N50  \\\n",
       "0    0.257834              120            0.009284   176   \n",
       "1    0.335338               83            0.006421  2250   \n",
       "2    0.401844               63            0.004874  3956   \n",
       "3    0.457034               50            0.003868  6450   \n",
       "4    0.503521               47            0.003636  6451   \n",
       "5    0.548069               44            0.003404  6452   \n",
       "6    0.585694               42            0.003249  6452   \n",
       "7    0.623002               39            0.003017  6453   \n",
       "8    0.654850               38            0.002940  6453   \n",
       "9    0.685314               37            0.002862  6453   \n",
       "10   0.713404               36            0.002785  6453   \n",
       "11   0.739753               33            0.002553  6453   \n",
       "12   0.764876               33            0.002553  6453   \n",
       "13   0.787625               32            0.002476  6453   \n",
       "14   0.807960               31            0.002398  6454   \n",
       "15   0.827109               29            0.002244  6455   \n",
       "16   0.844042               26            0.002011  6456   \n",
       "17   0.859274               26            0.002011  6456   \n",
       "18   0.876009               25            0.001934  6458   \n",
       "\n",
       "                                      component_sizes  \n",
       "0   [648, 570, 444, 404, 364, 338, 330, 313, 295, ...  \n",
       "1   [2869, 2481, 2250, 1288, 1203, 600, 599, 486, ...  \n",
       "2   [6450, 3956, 2494, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3   [6452, 6450, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4   [6453, 6451, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5   [6453, 6452, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "6   [6453, 6452, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7   [6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "8   [6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "9   [6453, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "10  [6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "11  [6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12  [6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "13  [6454, 6453, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "14  [6457, 6454, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15  [6457, 6455, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "16   [6458, 6456, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "17   [6458, 6456, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "18            [6459, 6458, 1, 1, 1, 1, 1, 1, 1, 1, 1]  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_simhash_array = get_simhash(ref_read_features,hash_table)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(ref_reads_simhash_array)\n",
    "indices = nbrs.kneighbors(que_reads_simhash_array,return_distance=False)\n",
    "print(\"done\\nevaluates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_table(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag,:]=_hash(kmer_index, seed=seed)\n",
    "            new_hash_table=np.reshape(hash_table,(kmer_num,3200))\n",
    "    return new_hash_table\n",
    "\n",
    "def _get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    *,\n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    repeat=100) -> Mapping[int,list]:  \n",
    "    all_read_simhash = []\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        weighted_hash_table = hash_table*idf[:, np.newaxis]\n",
    "    else:\n",
    "        weighted_hash_table = hash_table\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(weighted_hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(one_read_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "\n",
    "    return reads_simhash_array \n",
    "\n",
    "hash_table = _get_table(read_features,feature_matrix,seed = 15232,tf = False,idf=False,repeat=100)\n",
    "print(\"1\")\n",
    "reads_simhash_array = _get_simhash(read_features,feature_matrix,hash_table,seed = 15232,tf = True,idf=False,repeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int8) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat,32),dtype=np.int8) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty((kmer_num,32), dtype=np.int8) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index,:]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i,:] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "hash_table = mp_get_hashtable(read_features,feature_matrix,15232,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_read_hash = np.sum(hash_table[list(read_features.values())[0],:,:],axis=0)\n",
    "hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table=np.reshape(hash_table,(kmer_num,3200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_hash[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_read_intervals(meta_df):\n",
    "    read_intervals = {\n",
    "        i: [GenomicInterval(strand, start, end)]\n",
    "        for i, strand, start, end in zip(\n",
    "            meta_df.index,\n",
    "            meta_df[\"reference_strand\"],\n",
    "            meta_df[\"reference_start\"],\n",
    "            meta_df[\"reference_end\"],\n",
    "        )\n",
    "    }\n",
    "    return read_intervals\n",
    "\n",
    "read_intervals = get_read_intervals(meta_df)\n",
    "\n",
    "reference_graph = OverlapGraph.from_intervals(read_intervals)\n",
    "nr_edges = set((node_1, node_2) for node_1, node_2, data in reference_graph.edges(data=True) if not data['redundant'])\n",
    "connected_component_count = len(list(nx.connected_components(reference_graph)))\n",
    "len(reference_graph.nodes), len(reference_graph.edges), len(nr_edges), connected_component_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def mp_get_hashtable(  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    processes:int,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object) \n",
    "\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            seed = hash_seeds[i]\n",
    "            result = np.empty(kmer_num, dtype=object) \n",
    "            for kmer_index in range(kmer_num):\n",
    "                result[kmer_index]=_hash(kmer_index, seed=seed)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            hash_table[:,i] = result            \n",
    "\n",
    "        pool.map(work, range(repeat), reduce=reduce)\n",
    "    return hash_table\n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "## multi-process calculate simhash value\n",
    "def mp_get_simhash(\n",
    "    read_features: list,  \n",
    "    feature_matrix: list,\n",
    "    hash_table,\n",
    "    tf:bool,\n",
    "    idf:bool,\n",
    "    processes:int,):\n",
    "    \n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:list(set(v)) for k,v in read_features.items()} \n",
    "\n",
    "       \n",
    "    reads_simhash_array = np.empty((len(read_features),hash_table.shape[1]*32),dtype=object)\n",
    "    with sharedmem.MapReduce(np=processes) as pool:\n",
    "\n",
    "        def work(i):\n",
    "            read_kmer = list(read_features.values())[i]\n",
    "            one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "            conc_hash = np.concatenate(one_read_hash)\n",
    "            result = np.where(conc_hash > 0, 1, 0)\n",
    "            return i,result\n",
    "\n",
    "        def reduce(i, result):\n",
    "            reads_simhash_array[i,:] = result            \n",
    "        pool.map(work, range(len(read_features)), reduce=reduce)\n",
    "    return reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table = mp_get_hashtable(feature_matrix,repeat =100, seed = 4829,processes=12)\n",
    "print('hashtable established')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3\n",
    "\n",
    "for repeat in [100]:\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829,tf = True,idf = True)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 100\n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "all_read_simhash = []\n",
    "rng = np.random.default_rng(4829)  \n",
    "hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)\n",
    "kmer_num = feature_matrix.shape[1]\n",
    "hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "for flag,seed in enumerate(hash_seeds):\n",
    "    print(flag)\n",
    "    kmer_hash_indice = {} \n",
    "    for kmer_index in range(kmer_num):\n",
    "        hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "new_hash = hash_table*idf[:, np.newaxis]\n",
    "\n",
    "for read_ind,read_kmer in read_features.items():\n",
    "    one_read_hash = np.sum(new_hash[read_kmer,:],axis=0)\n",
    "    conc_hash = np.concatenate(one_read_hash)\n",
    "    simhash = np.where(conc_hash > 0, 1, 0)\n",
    "    all_read_simhash.append(simhash)\n",
    "reads_simhash_array = np.array(all_read_simhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_simhash_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(reads_simhash_array)  \n",
    "indices = nbrs.kneighbors(reads_simhash_array,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n",
    "\n",
    "read_ids = np.array(list(read_features))\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##带tf idf 初始版本\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)\n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32) \n",
    "    hash_array = np.where(hash_array == 0, -1, 1)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int,\n",
    "    tf:bool,\n",
    "    idf:bool,) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    all_read_simhash = []\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    hash_table = np.empty((kmer_num,repeat),dtype=object)  \n",
    "    for flag,seed in enumerate(hash_seeds):\n",
    "        for kmer_index in range(kmer_num):\n",
    "            hash_table[kmer_index,flag]=_hash(kmer_index, seed=seed)\n",
    "\n",
    "    if idf == True:\n",
    "        nested_list = list(read_features.values())\n",
    "        unrongh_nest = [list(set(sublist)) for sublist in nested_list]  \n",
    "        merged_list = list(chain.from_iterable(unrongh_nest)) \n",
    "        count = Counter(merged_list)\n",
    "        sorted_counts = dict(sorted(count.items(), key=lambda x: x[0]))  \n",
    "        times = np.array(list(sorted_counts.values()))\n",
    "        x = len(read_features)\n",
    "        arr = np.full(feature_matrix.shape[1],x)  \n",
    "        idf = np.log(arr/times)\n",
    "        hash_table = hash_table*idf[:, np.newaxis]\n",
    "    if tf == False:\n",
    "        read_features = {k:set(v) for k,v in read_features}\n",
    "\n",
    "    for read_kmer in read_features.values():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        conc_hash = np.concatenate(one_read_hash)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    reads_simhash_array = np.array(all_read_simhash)\n",
    "  \n",
    "    return reads_simhash_array \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##32位，70个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "\n",
    "    hash_seeds = np.array(range(repeat))\n",
    "    kmer_num = feature_matrix.shape[1]\n",
    "    kmer_index = np.array(range(kmer_num))\n",
    "\n",
    "    table = []\n",
    "    for seed in hash_seeds:\n",
    "        prime1 = 2654435761  # A large prime number\n",
    "        prime2 = 0x27d4eb2d  # Another large prime, often used in hashing\n",
    "        hash_value = (kmer_index * prime1) ^ (seed * prime2)\n",
    "        hash_t = hash_value % (2**32)\n",
    "        binary_matrix = np.vectorize(np.binary_repr)(hash_t, width=32)\n",
    "        one_repear_table = np.array([list(row) for row in binary_matrix.flatten()])\n",
    "        table.append(one_repear_table.astype(int))\n",
    "    hash_table = np.hstack(table)\n",
    "\n",
    "    all_read_simhash = []\n",
    "    for read_ind,read_kmer in read_features.items():\n",
    "        one_read_hash = np.sum(hash_table[read_kmer,:],axis=0)\n",
    "        simhash = np.where(conc_hash > 0, 1, 0)\n",
    "        all_read_simhash.append(simhash)\n",
    "    concat_simhash = np.array(all_read_simhash)\n",
    "    return concat_simhash \n",
    "\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "\n",
    "dfs = []\n",
    "for repeat in range(20,110,10):\n",
    "    print(repeat)\n",
    "    concat_simhash = _get_simhash(read_features,feature_matrix,repeat =repeat, seed = 4829)\n",
    "    nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "    nbrs.fit(concat_simhash)  \n",
    "    indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "    nbr_indices = indices[:, 1:]\n",
    "\n",
    "    for k in k_values:\n",
    "        graph = OverlapGraph.from_neighbor_indices(\n",
    "            neighbor_indices=nbr_indices,\n",
    "            n_neighbors=k,\n",
    "            read_ids=read_ids,\n",
    "            require_mutual_neighbors=False,\n",
    "        )\n",
    "        graphs[k] = graph\n",
    "\n",
    "    df_rows = []\n",
    "    for k in k_values:\n",
    "        graph = graphs[k]\n",
    "        graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "        stats = { \"n_neighbors\": k}\n",
    "        stats = {\"description\":'SimHash', \"n_neighbors\": k, \"repeat_time\": repeat,\n",
    "                    **graph_stats}\n",
    "        df_rows.append(stats)\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    dfs.append(df)\n",
    "    \n",
    "new = pd.concat(dfs)\n",
    "new.to_csv('/home/miaocj/docker_dir/test_simhash.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_table(tsv_path).iloc[:MAX_SAMPLE_SIZE, :].reset_index()\n",
    "read_indices = {read_name: read_id for read_id, read_name in meta_df['read_name'].items()}\n",
    "feature_matrix = sp.sparse.load_npz(npz_path)[meta_df.index, :]\n",
    "\n",
    "with gzip.open(json_path, \"rt\") as f:\n",
    "    read_features = json.load(f)\n",
    "    read_features = {i: read_features[i] for i in meta_df.index}\n",
    "\n",
    "feature_weights = {i: 1 for i in range(feature_matrix.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = np.array(list(read_features))\n",
    "graphs = collections.defaultdict(dict)\n",
    "k_values = np.arange(2, max_n_neighbors + 1)\n",
    "\n",
    "for k in k_values:\n",
    "    graph = OverlapGraph.from_neighbor_indices(\n",
    "        neighbor_indices=nbr_indices,\n",
    "        n_neighbors=k,\n",
    "        read_ids=read_ids,\n",
    "        require_mutual_neighbors=False,\n",
    "    )\n",
    "    graphs[k] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    graph = graphs[k]\n",
    "    graph_stats = get_overlap_statistics(query_graph=graph, reference_graph=reference_graph)\n",
    "    stats = { \"n_neighbors\": k}\n",
    "    stats = {\"description\":'SimHash', \"n_neighbors\": k, \n",
    "                 **graph_stats}\n",
    "    df_rows.append(stats)\n",
    "df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##70\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##40\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##20\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试hash方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def simhash(features):\n",
    "  \n",
    "  # Generate a hash for each feature\n",
    "  hashes = [hashlib.sha1(feature).hexdigest() for feature in features]\n",
    "  \n",
    "  # Combine the feature hashes to produce the final simhash\n",
    "  concatenated_hash = ''.join(hashes)\n",
    "  simhash = hashlib.sha1(concatenated_hash).hexdigest()\n",
    "  \n",
    "  return simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "x = np.random.rand(1024 * 1024 * 16)\n",
    "h = xxhash.xxh64()\n",
    "h.update(x); h.intdigest(); h.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 64位， 40个repeat\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:064b}\".format(hash_value & 0xFFFFFFFFFFFFFFFF)\n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]):  \n",
    "            kmer_index_str = str(kmer_index)  \n",
    "            hash_list = _hash(kmer_index_str, seed=s)  \n",
    "            kmer_hash_indice[kmer_index] = hash_list  \n",
    "  \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 40, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed)\n",
    "beta = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "x = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u1 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "u2 = rng.uniform(0, 1, (feature_count, dimension_count))\n",
    "\n",
    "for j_sample in range(0, sample_count):\n",
    "    feature_indices = sparse.find(data[:, j_sample] > 0)[0]\n",
    "    gamma = -np.log(np.multiply(u1[feature_indices, :], u2[feature_indices, :]))\n",
    "    t_matrix = np.floor(\n",
    "        np.divide(\n",
    "            matlib.repmat(\n",
    "                np.log(data[feature_indices, j_sample].todense()),\n",
    "                1,\n",
    "                dimension_count,\n",
    "            ),\n",
    "            gamma,\n",
    "        )\n",
    "        + beta[feature_indices, :]\n",
    "    )\n",
    "    y_matrix = np.exp(np.multiply(gamma, t_matrix - beta[feature_indices, :]))\n",
    "    a_matrix = np.divide(\n",
    "        -np.log(x[feature_indices, :]),\n",
    "        np.divide(y_matrix, u1[feature_indices, :]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##建立hash矩阵\n",
    "import numpy as np  \n",
    "import collections  \n",
    "from typing import Mapping  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from scipy.spatial.distance import hamming  \n",
    "import mmh3  \n",
    "\n",
    "def _hash(kmer_index: int, seed: int) -> np.ndarray:  \n",
    "    hash_value = mmh3.hash(str(kmer_index), seed=seed)  \n",
    "    binary_string = \"{0:032b}\".format(hash_value & 0xFFFFFFFF)  \n",
    "    hash_array = np.array([int(x) for x in binary_string], dtype=np.int32)  \n",
    "    return hash_array  \n",
    "\n",
    "def _get_simhash(  \n",
    "    read_features: list,  \n",
    "    feature_matrix: list,   \n",
    "    repeat: int,   \n",
    "    seed: int) -> Mapping[int,list]:  \n",
    "      \n",
    "    rng = np.random.default_rng(seed)  \n",
    "    hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeat, dtype=np.uint64)  \n",
    "  \n",
    "    repeat_all_read_hash = []  \n",
    "    hash_table = np.empty((repeat,feature_matrix.shape[1]))  \n",
    "    for s in hash_seeds:  \n",
    "        print(s)\n",
    "        kmer_hash_indice = {}  \n",
    "        for kmer_index in range(feature_matrix.shape[1]): \n",
    "            hash_table[s,kmer_index] = _hash(kmer_index, seed=s)\n",
    "    for \n",
    "         \n",
    "        all_read_simhash = []  \n",
    "        for features in read_features.values():  \n",
    "            feature_count = dict(collections.Counter(features))  \n",
    "            one_read_hash = []  \n",
    "            for indice, count in feature_count.items():  \n",
    "                hash_list = kmer_hash_indice[indice]\n",
    "                hash_list = np.where(hash_list == 0, -1, hash_list) \n",
    "                weighted_hash_list = hash_list * count  \n",
    "                one_read_hash.append(weighted_hash_list)  \n",
    "            one_read_hash_array = np.array(one_read_hash)  \n",
    "            hash_sum = np.sum(one_read_hash_array, axis=0)   \n",
    "            simhash_value = np.where(hash_sum > 0, 1, 0)  \n",
    "            all_read_simhash.append(simhash_value)  \n",
    "  \n",
    "        repeat_all_read_hash.append(all_read_simhash)  \n",
    "  \n",
    "    concat_simhash = np.concatenate(repeat_all_read_hash, axis=1)  \n",
    "  \n",
    "    return concat_simhash \n",
    "\n",
    "concat_simhash = _get_simhash(read_features,feature_matrix,repeat = 70, seed = 4829)\n",
    "def hamming_distance(x, y):  \n",
    "    return np.count_nonzero(x != y)\n",
    "nbrs = NearestNeighbors(n_neighbors=21, algorithm='auto', metric=hamming_distance)\n",
    "nbrs.fit(concat_simhash)  \n",
    "indices = nbrs.kneighbors(concat_simhash,return_distance=False)\n",
    "nbr_indices = indices[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
