{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from snakemake_stub import *\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import gzip, json, collections\n",
    "from typing import Sequence, Mapping, Collection\n",
    "from Bio import SeqIO\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "def init_reverse_complement():\n",
    "    TRANSLATION_TABLE = str.maketrans(\"ACTGactg\", \"TGACtgac\")\n",
    "\n",
    "    def reverse_complement(sequence: str) -> str:\n",
    "        \"\"\"\n",
    "        >>> reverse_complement(\"AATC\")\n",
    "        'GATT'\n",
    "        >>> reverse_complement(\"CCANT\")\n",
    "        'ANTGG'\n",
    "        \"\"\"\n",
    "        sequence = str(sequence)\n",
    "        return sequence.translate(TRANSLATION_TABLE)[::-1]\n",
    "\n",
    "    return reverse_complement\n",
    "\n",
    "\n",
    "reverse_complement = init_reverse_complement()\n",
    "import mmh3\n",
    "import sharedmem\n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "import numpy as np  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import anndata\n",
    "\n",
    "class _SpectralMatrixFree:\n",
    "    \"\"\"\n",
    "    Perform dimension reduction using Laplacian Eigenmaps.\n",
    "\n",
    "    Matrix-free spectral embedding without computing the similarity matrix explicitly.\n",
    "\n",
    "    Only cosine similarity is supported.\n",
    "\n",
    "    Adapted from https://github.com/kaizhang/SnapATAC2/blob/51f040c095820fea43e9a6360d751bfc29faecc5/snapatac2-python/python/snapatac2/tools/_embedding.py#L434\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dim: int = 30,\n",
    "        feature_weights=None,\n",
    "    ):\n",
    "        self.out_dim = out_dim\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    def fit(self, mat):\n",
    "        if self.feature_weights is not None:\n",
    "            mat = mat @ sp.sparse.diags(self.feature_weights)\n",
    "        self.sample = mat\n",
    "        self.in_dim = mat.shape[1]\n",
    "\n",
    "        s = 1 / np.sqrt(np.ravel(sp.sparse.csr_matrix.power(mat, 2).sum(axis=1)))\n",
    "        X = sp.sparse.diags(s) @ mat\n",
    "\n",
    "        D = np.ravel(X @ X.sum(axis=0).T) - 1\n",
    "        X = sp.sparse.diags(1 / np.sqrt(D)) @ X\n",
    "        evals, evecs = self._eigen(X, 1 / D, k=self.out_dim)\n",
    "\n",
    "        ix = evals.argsort()[::-1]\n",
    "        self.evals = evals[ix]\n",
    "        self.evecs = evecs[:, ix]\n",
    "\n",
    "    def transform(self, weighted_by_sd: bool = True):\n",
    "        evals = self.evals\n",
    "        evecs = self.evecs\n",
    "\n",
    "        if weighted_by_sd:\n",
    "            idx = [i for i in range(evals.shape[0]) if evals[i] > 0]\n",
    "            evals = evals[idx]\n",
    "            evecs = evecs[:, idx] * np.sqrt(evals)\n",
    "        return evals, evecs\n",
    "\n",
    "    @staticmethod\n",
    "    def _eigen(X, D, k):\n",
    "        def f(v):\n",
    "            return X @ (v.T @ X).T - D * v\n",
    "\n",
    "        n = X.shape[0]\n",
    "        A = sp.sparse.linalg.LinearOperator((n, n), matvec=f, dtype=np.float64)\n",
    "        return sp.sparse.linalg.eigsh(A, k=k)\n",
    "\n",
    "class _DimensionReduction:\n",
    "\n",
    "    def transform(self, data: csr_matrix | NDArray, n_dimensions: int) -> NDArray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectralEmbedding(_DimensionReduction):\n",
    "    def transform(\n",
    "        self, data: csr_matrix | NDArray, n_dimensions: int, weighted_by_sd: bool = True\n",
    "    ) -> NDArray:\n",
    "        reducer = _SpectralMatrixFree(out_dim=n_dimensions)\n",
    "        reducer.fit(data)\n",
    "        _, embedding = reducer.transform(weighted_by_sd=weighted_by_sd)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('/home/miaocj/docker_dir/kNN-overlap-finder/data/evaluation/metagenome/pbsim_ONT_95_20k/kmer_16/HNSW_Cosine_Spectural_500d_IDF_nbr_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81663], dtype=uint64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['arr_0'][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_database = '/home/miaocj/docker_dir/kNN-overlap-finder/data/metagenome_reference/GCR.fa' ## include 1310 species \n",
    "origin_ref_datebase = '/home/miaocj/docker_dir/kNN-overlap-finder/data/metagenome_reference/GCR.fa.split/GCR.part_001.fa' \n",
    "##used for generate simulated reads, include 19 species\n",
    "query_reads = '/home/miaocj/docker_dir/kNN-overlap-finder/data/metagenome_reads/part001/pbsim_ONT_95_20k/reads.fa'\n",
    "\n",
    "##Part1: generate a read-species dict, using for final examination\n",
    "all_ref_reads_tax_list = []\n",
    "with open(all_ref_database, \"rt\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        tax=record.id[:6]\n",
    "        all_ref_reads_tax_list.append(tax)##ID example: QSQV01000067.1, 'QSQV01' present species code\n",
    "all_ref_read_tax = {i:tax for i,tax in enumerate(all_ref_reads_tax_list)}\n",
    "\n",
    "\n",
    "ref_reads_tax_list = []\n",
    "with open(origin_ref_datebase, \"rt\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        tax=record.id[:6]\n",
    "        ref_reads_tax_list.append(tax)##ID example: QSQV01000067.1, 'QSQV01' present species code\n",
    "ref_read_tax = {i:tax for i,tax in enumerate(ref_reads_tax_list)}\n",
    "\n",
    "flag = 0\n",
    "que_read_tax = {}\n",
    "with open(query_reads) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            start = lines.index('S')\n",
    "            end = lines.index('_')\n",
    "            ref_num = lines[start+1:end]\n",
    "            que_read_tax[flag] = ref_read_tax[int(ref_num)-1]\n",
    "            flag +=1\n",
    "que_read_tax_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/que_read_tax.json'\n",
    "ref_read_tax_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/ref_read_tax.json'\n",
    "\n",
    "with gzip.open(que_read_tax_file, \"wt\") as f:\n",
    "    json.dump(que_read_tax, f)\n",
    "with gzip.open(ref_read_tax_file, \"wt\") as f:\n",
    "    json.dump(all_ref_read_tax, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_read_tax_file ='/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/ref_read_tax.json.gz'\n",
    "que_read_tax_file ='/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/que_read_tax.json.gz'\n",
    "ref_feature_matrix_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/que_feature_matrix.npz'\n",
    "que_feature_matrix_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/ref_feature_matrix.npz'\n",
    "\n",
    "with gzip.open(ref_read_tax_file, \"rt\") as f:\n",
    "    ref_read_tax = json.load(f)\n",
    "with gzip.open(que_read_tax_file, \"rt\") as f:\n",
    "    que_read_tax = json.load(f)\n",
    "\n",
    "ref_feature_matrix = sp.sparse.load_npz(ref_feature_matrix_file)\n",
    "que_feature_matrix = sp.sparse.load_npz(que_feature_matrix_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as normalize_function\n",
    "\n",
    "class scBiMapEmbedding(_DimensionReduction):\n",
    "    \"\"\"\n",
    "    From scBiMapping on Conda\n",
    "    Author: Teng Qiu\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(\n",
    "        self,\n",
    "        data: csr_matrix | NDArray,\n",
    "        n_dimensions: int,\n",
    "        *,\n",
    "        normalize=True,\n",
    "    ) -> NDArray:\n",
    "        if data.min() < 0:\n",
    "            raise ValueError(\n",
    "                \"The input matrix is regarded as a similarity matrix and thus should not contain negtive values\"\n",
    "            )\n",
    "        eps=0.0000000001,\n",
    "        Dx = sp.sparse.diags(np.ravel(1 / (data.sum(axis=1) + eps)))\n",
    "\n",
    "        y = (Dx @ csr_matrix(data.sum(axis=1))).T  # row vector\n",
    "        Dy = sp.sparse.diags(np.ravel((1 / (np.sqrt((y @ data).T.toarray()) + eps))))  # type: ignore\n",
    "        C = np.sqrt(Dx) @ data @ Dy  # type: ignore\n",
    "        _, _, evec = sp.sparse.linalg.svds( # type: ignore\n",
    "            C, k=n_dimensions, return_singular_vectors=\"vh\", random_state=0,\n",
    "        )\n",
    "        V = Dy @ evec.T  # eigenvectors for features\n",
    "        U = Dx @ data @ V  # eigenvectors for cells\n",
    "\n",
    "        if normalize:\n",
    "            U = normalize_function(U, axis=1, norm=\"l2\")\n",
    "        return U  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/site-packages/scipy/__init__.py'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_array\n",
    "from scipy.sparse.linalg import svds\n",
    "mat = csc_array((65536, 65536))\n",
    "_, s, _ = svds(mat)\n",
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "\n",
    "class LowHash():\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash(x: int, seed: int) -> int:\n",
    "        hash_value = mmh3.hash(str(x), seed=int(seed))\n",
    "        return hash_value\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hash_values(data: Iterable[int], repeats: int, seed: int) -> np.ndarray:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeats, dtype=np.uint64)\n",
    "        hash_values = []\n",
    "        for k in range(repeats):\n",
    "            s = hash_seeds[k]\n",
    "            for x in data:\n",
    "                hash_values.append(LowHash._hash(x, seed=s))\n",
    "        hash_values = np.array(hash_values, dtype=np.int64)\n",
    "        return hash_values\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lowhash_count(\n",
    "        hash_count: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "    ) -> int:\n",
    "        if lowhash_fraction is None and lowhash_count is None:\n",
    "            raise TypeError(\n",
    "                \"Either `lowhash_fraction` or `lowhash_count` must be specified.\"\n",
    "            )\n",
    "        if lowhash_fraction is not None and lowhash_count is not None:\n",
    "            raise TypeError(\n",
    "                f\"`lowhash_fraction` and `lowhash_count` cannot be specified at the same time. {lowhash_fraction=} {lowhash_count=}\"\n",
    "            )\n",
    "\n",
    "        if lowhash_fraction is not None:\n",
    "            lowhash_count = ceil(hash_count * lowhash_fraction)\n",
    "            lowhash_count = max(lowhash_count, 1)\n",
    "        if lowhash_count is None:\n",
    "            raise ValueError()\n",
    "        return lowhash_count\n",
    "\n",
    "    def _lowhash(\n",
    "        \n",
    "        data: csr_matrix | np.ndarray,\n",
    "        repeats: int,\n",
    "        lowhash_fraction: float | None,\n",
    "        lowhash_count: int | None = None,\n",
    "        seed: int = 5731343,\n",
    "        verbose=True,\n",
    "    ) -> csr_matrix:\n",
    "        sample_count, feature_count = data.shape\n",
    "        buckets = sp.sparse.dok_matrix(\n",
    "            (feature_count * repeats, sample_count), dtype=np.bool_\n",
    "        )\n",
    "\n",
    "        # Calculate hash values\n",
    "        hash_values = self._get_hash_values(\n",
    "            np.arange(feature_count), repeats=repeats, seed=seed\n",
    "        )\n",
    "\n",
    "        # For each sample, find the lowest hash values for its features\n",
    "        for j in range(sample_count):\n",
    "            feature_indices = sp.sparse.find(data[j, :] > 0)[1]\n",
    "            hash_count = feature_indices.shape[0]\n",
    "            sample_lowhash_count = self._get_lowhash_count(\n",
    "                hash_count=hash_count,\n",
    "                lowhash_fraction=lowhash_fraction,\n",
    "                lowhash_count=lowhash_count,\n",
    "            )\n",
    "            for k in range(repeats):\n",
    "                bucket_indices = feature_indices + (k * feature_count)\n",
    "                sample_hash_values = hash_values[bucket_indices]\n",
    "                low_hash_buckets = bucket_indices[\n",
    "                    np.argsort(sample_hash_values)[:sample_lowhash_count]\n",
    "                ]\n",
    "                buckets[low_hash_buckets, j] = 1\n",
    "            if verbose and j % 1000 == 0:\n",
    "                print(j, end=\" \")\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        buckets = sp.sparse.csr_matrix(buckets)\n",
    "        return buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 43000 44000 45000 46000 47000 48000 49000 50000 51000 52000 53000 54000 55000 56000 57000 58000 59000 60000 61000 62000 63000 64000 65000 66000 67000 68000 69000 70000 71000 72000 73000 74000 75000 76000 77000 78000 79000 80000 81000 82000 83000 84000 85000 86000 87000 88000 89000 90000 91000 92000 93000 94000 95000 96000 97000 98000 99000 100000 101000 102000 103000 104000 105000 106000 107000 108000 109000 110000 111000 112000 113000 114000 115000 116000 117000 118000 119000 120000 121000 122000 123000 124000 125000 126000 127000 128000 129000 130000 131000 132000 133000 134000 135000 136000 137000 138000 139000 140000 141000 142000 143000 144000 145000 146000 147000 148000 149000 150000 151000 152000 153000 154000 155000 156000 157000 158000 159000 160000 161000 162000 163000 164000 165000 166000 167000 168000 169000 170000 171000 172000 173000 174000 175000 176000 177000 178000 179000 180000 181000 182000 183000 184000 185000 186000 187000 188000 189000 190000 191000 192000 193000 194000 195000 196000 197000 198000 199000 200000 201000 202000 203000 204000 205000 206000 207000 208000 209000 210000 211000 212000 213000 214000 215000 216000 217000 218000 219000 220000 221000 222000 223000 224000 225000 226000 227000 228000 229000 230000 231000 232000 233000 234000 235000 236000 237000 238000 239000 240000 241000 242000 243000 244000 245000 246000 247000 248000 249000 250000 251000 252000 253000 254000 255000 256000 257000 258000 259000 260000 261000 262000 263000 264000 265000 266000 267000 268000 269000 270000 271000 272000 273000 274000 275000 276000 277000 278000 279000 280000 281000 282000 283000 284000 285000 286000 287000 288000 289000 290000 291000 292000 293000 294000 295000 296000 297000 298000 299000 300000 301000 302000 303000 304000 305000 306000 307000 308000 309000 310000 311000 312000 313000 314000 315000 316000 317000 318000 319000 320000 321000 322000 323000 324000 325000 326000 327000 328000 329000 330000 331000 332000 \n"
     ]
    }
   ],
   "source": [
    "data = sp.sparse.vstack([ref_feature_matrix,que_feature_matrix])\n",
    "buckets = LowHash()._lowhash(data,repeats=100,lowhash_fraction=0.01,lowhash_count=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = buckets.sum(axis=1).A1  # type: ignore\n",
    "matrix = buckets[\n",
    "    (row_sums >= 2) & (row_sums <= float(\"inf\")), :\n",
    "].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15396476, 332922)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1078533000, 332922)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332922, 10785330)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "svds() got an unexpected keyword argument 'lapack_driver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mvstack([ref_feature_matrix,que_feature_matrix])\n\u001b[1;32m      2\u001b[0m ref_read_num \u001b[38;5;241m=\u001b[39m ref_feature_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m _data \u001b[38;5;241m=\u001b[39m \u001b[43mscBiMapEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_dimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mscBiMapEmbedding.transform\u001b[0;34m(self, data, n_dimensions, normalize)\u001b[0m\n\u001b[1;32m     24\u001b[0m Dy \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mdiags(np\u001b[38;5;241m.\u001b[39mravel((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39msqrt((y \u001b[38;5;241m@\u001b[39m data)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mtoarray()) \u001b[38;5;241m+\u001b[39m eps))))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     25\u001b[0m C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(Dx) \u001b[38;5;241m@\u001b[39m data \u001b[38;5;241m@\u001b[39m Dy  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m _, _, evec \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_singular_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapack_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgesvd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m V \u001b[38;5;241m=\u001b[39m Dy \u001b[38;5;241m@\u001b[39m evec\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# eigenvectors for features\u001b[39;00m\n\u001b[1;32m     30\u001b[0m U \u001b[38;5;241m=\u001b[39m Dx \u001b[38;5;241m@\u001b[39m data \u001b[38;5;241m@\u001b[39m V  \u001b[38;5;66;03m# eigenvectors for cells\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: svds() got an unexpected keyword argument 'lapack_driver'"
     ]
    }
   ],
   "source": [
    "data = sp.sparse.vstack([ref_feature_matrix,que_feature_matrix])\n",
    "ref_read_num = ref_feature_matrix.shape[0]\n",
    "_data = scBiMapEmbedding().transform(data,n_dimensions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=0.0000000001,\n",
    "Dx = sp.sparse.diags(np.ravel(1 / (data.sum(axis=1) + eps)))\n",
    "\n",
    "y = (Dx @ csr_matrix(data.sum(axis=1))).T  # row vector\n",
    "Dy = sp.sparse.diags(np.ravel((1 / (np.sqrt((y @ data).T.toarray()) + eps))))  # type: ignore\n",
    "C = np.sqrt(Dx) @ data @ Dy  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ref = _data[:ref_read_num]\n",
    "_que = _data[ref_read_num:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = x['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250507"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "prediction = []\n",
    "for query_read_num,x in enumerate(indice):\n",
    "    neighbor = x[0]\n",
    "    neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "    query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "    prediction.append(ref_read_tax[str(int(neighbor))])\n",
    "    actual.append(que_read_tax[str(int(query_read_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_sep = precision_score(actual, prediction, average=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypre=[]\n",
    "mysen=[]\n",
    "for i in set(actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for x in range(len(actual)):\n",
    "        if actual[x] == i:\n",
    "            if actual[x] == prediction[x]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if prediction[x] == i:\n",
    "                fp += 1\n",
    "    pre = tp/(tp+fp)\n",
    "    sen = tp/(tp+fn)\n",
    "    mypre.append(pre)\n",
    "    mysen.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651967756731813"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mypre)/len(mypre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(precision_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indice,ref_read_tax,que_read_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04656866497957633"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(precision_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate sensitivity and precision\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score  \n",
    "\n",
    "def evaluate(indices,ref_read_tax,que_read_tax):\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    for query_read_num,x in enumerate(indices):\n",
    "        neighbor = x[0]\n",
    "        neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "        query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "        prediction.append(ref_read_tax[str(int(neighbor))])\n",
    "        actual.append(que_read_tax[str(int(query_read_num))])\n",
    "\n",
    "    precision = precision_score(actual, prediction,average='macro')\n",
    "    sensitivity = recall_score(actual, prediction,average='macro')\n",
    "    ##计算每个类别的\n",
    "    precision_sep = precision_score(actual, prediction, average=None)  \n",
    "    sensitivity_sep = recall_score(actual, prediction, average=None)\n",
    "    return precision,sensitivity,precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_database = '/home/miaocj/docker_dir/data/metagenome/part1.fa'\n",
    "query_reads = '/home/miaocj/docker_dir/data/metagenome/pbsim_ONT_95_30k_10dep_part1_reads.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_tax_list = []\n",
    "with open(ref_database) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            line = lines.strip().split(' ')\n",
    "            ref_reads_tax_list.append(line[1]+line[2])\n",
    "ref_read_tax = {i:tax for i,tax in enumerate(ref_reads_tax_list)}\n",
    "\n",
    "flag = 0\n",
    "que_read_tax = {}\n",
    "with open(query_reads) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            start = lines.index('S')\n",
    "            end = lines.index('_')\n",
    "            ref_num = lines[start+1:end]\n",
    "            que_read_tax[flag] = ref_read_tax[int(ref_num)-1]\n",
    "            flag +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_read_tax_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/bacteria/que_read_tax.json'\n",
    "ref_read_tax_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/bacteria/ref_read_tax.json'\n",
    "\n",
    "with gzip.open(que_read_tax_file, \"wt\") as f:\n",
    "    json.dump(que_read_tax, f)\n",
    "with gzip.open(ref_read_tax_file, \"wt\") as f:\n",
    "    json.dump(ref_read_tax, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reads(fasta_path: str):\n",
    "    read_sequences = []\n",
    "    read_names = []\n",
    "    read_orientations = []\n",
    "\n",
    "    with open(fasta_path, \"rt\") as handle:  # Open gzipped file in text mode\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            read_sequences.append(seq)\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"+\")\n",
    "\n",
    "            # Include reverse complement\n",
    "            read_sequences.append(reverse_complement(seq))\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"-\")\n",
    "\n",
    "    return read_names, read_orientations, read_sequences\n",
    "\n",
    "read_names, read_orientations, read_sequences = load_reads(ref_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 276256424, 2: 23715451, 3: 3702956, 4: 1039535, 5: 423488, 6: 221958, 7: 134826, 8: 86799, 9: 59680, 10: 48269})\n"
     ]
    }
   ],
   "source": [
    "def build_kmer_index(\n",
    "    read_sequences: Sequence[str],\n",
    "    k: int,\n",
    "    *,\n",
    "    sample_fraction: float,\n",
    "    min_multiplicity: int,\n",
    "    seed: int,\n",
    ") -> Mapping[str, int]:\n",
    "    kmer_counter = collections.Counter()\n",
    "    for seq in read_sequences:\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            kmer_counter[kmer] += 1\n",
    "\n",
    "    kmer_spectrum = collections.Counter(x for x in kmer_counter.values() if x <= 10)\n",
    "    print(kmer_spectrum)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    vocabulary = set(\n",
    "        x\n",
    "        for x, count in kmer_counter.items()\n",
    "        if count >= min_multiplicity and rng.random() <= sample_fraction\n",
    "    )\n",
    "    vocabulary |= set(reverse_complement(x) for x in vocabulary)\n",
    "    kmer_indices = {kmer: i for i, kmer in enumerate(vocabulary)}\n",
    "    return kmer_indices\n",
    "\n",
    "sample_fraction=0.1\n",
    "min_multiplicity=2\n",
    "seed=562104830\n",
    "kmer_indices = build_kmer_index(        \n",
    "        read_sequences=read_sequences,\n",
    "        k=16,\n",
    "        sample_fraction=sample_fraction,\n",
    "        min_multiplicity=min_multiplicity,\n",
    "        seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qread_names, qread_orientations, qread_sequences = load_reads(query_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(\n",
    "    read_sequences: Sequence[str],\n",
    "    kmer_indices: Mapping[str, int],\n",
    "    k: int,\n",
    ") -> tuple[sp.csr_matrix, Sequence[Sequence[int]]]:\n",
    "    row_ind, col_ind, data = [], [], []\n",
    "    read_features = []\n",
    "    for i, seq in enumerate(read_sequences):\n",
    "        features_i = []\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            j = kmer_indices.get(kmer)\n",
    "            if j is None:\n",
    "                continue\n",
    "            features_i.append(j)\n",
    "\n",
    "        read_features.append(features_i)\n",
    "\n",
    "        kmer_counts = collections.Counter(features_i)\n",
    "        for j, count in kmer_counts.items():\n",
    "            row_ind.append(i)\n",
    "            col_ind.append(j)\n",
    "            data.append(count)\n",
    "\n",
    "    feature_matrix = sp.csr_matrix(\n",
    "        (data, (row_ind, col_ind)), shape=(len(read_sequences), len(kmer_indices))\n",
    "    )\n",
    "    return feature_matrix, read_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feature_matrix,ref_read_features = build_feature_matrix(read_sequences=read_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5647520)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isal import igzip\n",
    "def open_gzipped(path, mode=\"rt\", gzipped: bool | None = None, **kw):\n",
    "    if gzipped is None:\n",
    "        gzipped = path.endswith(\".gz\")\n",
    "    if gzipped:\n",
    "        open_ = igzip.open\n",
    "        return open_(path, mode)\n",
    "    else:\n",
    "        open_ = open\n",
    "    return open_(path, mode, **kw)\n",
    "                   \n",
    "output_npz_file = '/home/miaocj/docker_dir/data/metagenome/ref_feature_matrix.npz'\n",
    "output_json_file = '/home/miaocj/docker_dir/data/metagenome/ref_read_features.json.gz'\n",
    "sp.save_npz(output_npz_file, ref_feature_matrix)\n",
    "with open_gzipped(output_json_file, \"wt\") as f:\n",
    "    json.dump(ref_read_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_feature_matrix,que_read_features = build_feature_matrix(read_sequences=qread_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##存储\n",
    "output_npz_file2 = '/home/miaocj/docker_dir/data/metagenome/que_feature_matrix.npz'\n",
    "output_json_file2 = '/home/miaocj/docker_dir/data/metagenome/que_read_features.json.gz'\n",
    "sp.save_npz(output_npz_file2, que_feature_matrix)\n",
    "with open_gzipped(output_json_file2, \"wt\") as f:\n",
    "    json.dump(que_read_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ref_npz_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/ref_feature_matrix.npz'\n",
    "output_que_npz_file= '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/que_feature_matrix.npz'\n",
    "output_ref_json_file= '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/ref_read_features.json.gz.npz'\n",
    "output_que_json_file = '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/pbsim_ONT_95_20k/kmer_16/que_read_features.json.gz'\n",
    "\n",
    "# with gzip.open(output_json_file2, \"rt\") as f:\n",
    "#     que_read_features = json.load(f)\n",
    "ref_feature_matrix = sp.load_npz(output_ref_npz_file)\n",
    "que_feature_matrix = sp.load_npz(output_que_npz_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读取\n",
    "output_npz_file2 = '/home/miaocj/docker_dir/data/metagenome/que_feature_matrix.npz'\n",
    "output_npz_file = '/home/miaocj/docker_dir/data/metagenome/ref_feature_matrix.npz'\n",
    "output_json_file2 = '/home/miaocj/docker_dir/data/metagenome/que_read_features.json.gz'\n",
    "ref_feature_matrix = sp.sparse.load_npz(output_npz_file)\n",
    "que_feature_matrix = sp.sparse.load_npz(output_npz_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2899125)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119250, 2899125)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =sp.vstack([ref_feature_matrix, que_feature_matrix])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332922, 10785330)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_read_num = ref_feature_matrix.shape[0]\n",
    "\n",
    "_ref = data[:ref_read_num]\n",
    "_que = data[ref_read_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252666, 10785330)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "from numpy import matlib, ndarray\n",
    "from numpy.typing import NDArray\n",
    "import sklearn.neighbors\n",
    "\n",
    "class _NearestNeighbors:\n",
    "    def get_neighbors(\n",
    "        self, ref: csr_matrix | np.ndarray, que: csr_matrix | np.ndarray, n_neighbors: int\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class ExactNearestNeighbors(_NearestNeighbors):\n",
    "    def get_neighbors(\n",
    "        self, ref: csr_matrix | np.ndarray, que: csr_matrix | np.ndarray, metric=\"cosine\", n_neighbors: int = 20\n",
    "    ):\n",
    "        \n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=1, algorithm='auto',metric=metric).fit(ref)  \n",
    "        _, nbr_indices = nbrs.kneighbors(que)\n",
    "        return nbr_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact=ExactNearestNeighbors()\n",
    "nbr=exact.get_neighbors(ref_feature_matrix,que_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feature_matrix1= sp.load_npz(output_ref_npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252666, 10785330)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  252666, 10785330])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   80256, 10785330])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_feature_matrix['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile '/home/miaocj/docker_dir/kNN-overlap-finder/data/feature_matrix/metagenome/ref_feature_matrix.npz' with keys: indices, indptr, format, shape, data)\n"
     ]
    }
   ],
   "source": [
    "print(que_feature_matrix.keys())\n",
    "sparse_data = sp.csr_matrix(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12485670)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读取\n",
    "output_npz_file2 = '/home/miaocj/docker_dir/data/metagenome/que_feature_matrix.npz'\n",
    "output_json_file2 = '/home/miaocj/docker_dir/data/metagenome/que_read_features.json.gz'\n",
    "with gzip.open(output_json_file2, \"rt\") as f:\n",
    "    que_read_features = json.load(f)\n",
    "que_feature_matrix = sp.sparse.load_npz(output_npz_file2)\n",
    "\n",
    "output_npz_file = '/home/miaocj/docker_dir/data/metagenome/ref_feature_matrix.npz'\n",
    "output_json_file = '/home/miaocj/docker_dir/data/metagenome/ref_read_features.json.gz'\n",
    "with gzip.open(output_json_file, \"rt\") as f:\n",
    "    ref_read_features = json.load(f)\n",
    "ref_feature_matrix = sp.sparse.load_npz(output_npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119250, 5647520)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_indices = [index for index, sublist in enumerate(que_read_features) if len(sublist) == 0]  \n",
    "empty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/3172702e82c9c1d9450fdd20452651b9_/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-12 06:21:10.226389: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 06:21:11.465526: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../../scripts\")\n",
    "merged_matrix = sp.sparse.vstack([ref_feature_matrix, que_feature_matrix])  \n",
    "from dim_reduction import scBiMapEmbedding\n",
    "dim500 = scBiMapEmbedding().transform(merged_matrix,n_dimensions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim500_ref = dim500[:1000]\n",
    "dim500_que = dim500[1001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 降维+精确求解\n",
    "\n",
    "query_vectors = np.array(dim500_que) \n",
    "database_vectors = np.array(dim500_ref) \n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto',metric='cosine').fit(database_vectors)  \n",
    "distances, indices = nbrs.kneighbors(query_vectors)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.970391944806762, 0.9575761776138907)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categories = []\n",
    "for i in range(1,13):\n",
    "    categories.append(i)\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=300) \n",
    "plt.bar(categories,precision_sep,color = 'gray')\n",
    "\n",
    "plt.ylabel('Genus Precision')\n",
    "plt.ylim(0.6,1.01)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.gca().spines['right'].set_color('none')  \n",
    "plt.gca().spines['top'].set_color('none') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "plt.figure(figsize=(6, 6), dpi=300) \n",
    "plt.scatter(precision_sep,sensitivity_sep, label='Data points', marker='o', c=sensitivity_sep, cmap='rainbow')\n",
    "\n",
    "plt.xlim(0.8,1.01)\n",
    "plt.ylim(0.8,1.01)\n",
    "plt.gca().spines['right'].set_color('none')  \n",
    "plt.gca().spines['top'].set_color('none') \n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.margins(0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9584399030599837\n"
     ]
    }
   ],
   "source": [
    "##rough evaluate\n",
    "right_num = 0\n",
    "for query_read_num,x in enumerate(indices):\n",
    "    neighbor = x[0]\n",
    "    neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "    query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "    if ref_read_tax[neighbor] == que_read_tax[query_read_num]:\n",
    "        right_num +=1\n",
    "\n",
    "print(right_num/len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87046535, 0.62569832, 0.83373867, 0.86720252, 0.89721499,\n",
       "        0.87860642, 0.86374657, 0.41751232, 0.90688083, 0.54479376,\n",
       "        0.83079625, 0.93578767]),\n",
       " array([0.47510944, 0.98778833, 0.61177807, 0.68116937, 0.66164303,\n",
       "        0.6721266 , 0.60990641, 0.99246462, 0.41897312, 0.68407741,\n",
       "        0.71957404, 0.33051104]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "\n",
    "class HNSW():\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        ref_data: csr_matrix | np.ndarray,\n",
    "        que_data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        metric: Literal[\"euclidean\", \"cosine\"] = \"euclidean\",\n",
    "        *,\n",
    "        threads: int | None = None,\n",
    "        M: int = 16,\n",
    "        ef_construction: int = 200,\n",
    "        ef_search: int = 50,\n",
    "    ) -> np.ndarray:\n",
    "        if metric == \"euclidean\":\n",
    "            space = \"l2\"\n",
    "        else:\n",
    "            space = metric\n",
    "\n",
    "        # Initialize the HNSW index\n",
    "        p = hnswlib.Index(space=space, dim=ref_data.shape[1])\n",
    "        if threads is not None:\n",
    "            p.set_num_threads(threads)\n",
    "        p.init_index(max_elements=ref_data.shape[0], ef_construction=ef_construction, M=M)\n",
    "        ids = np.arange(ref_data.shape[0])\n",
    "        p.add_items(ref_data, ids)\n",
    "        p.set_ef(ef_search)\n",
    "        nbr_indices, _ = p.knn_query(que_data, k=n_neighbors)\n",
    "        return nbr_indices\n",
    "nbr_indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dim500_HNSW\n",
    "\n",
    "indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9581952089324665, 0.952445601680645)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "import collections\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy import sparse\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import mmh3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LowHash():\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash(x: int, seed: int) -> int:\n",
    "        hash_value = mmh3.hash(str(x), seed=seed)\n",
    "        return hash_value\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hash_values(data: Iterable[int], repeats: int, seed: int) -> np.ndarray:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeats, dtype=np.uint64)\n",
    "        hash_values = []\n",
    "        for k in range(repeats):\n",
    "            s = hash_seeds[k]\n",
    "            for x in data:\n",
    "                hash_values.append(LowHash._hash(x, seed=s))\n",
    "        hash_values = np.array(hash_values, dtype=np.int64)\n",
    "        return hash_values\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lowhash_count(\n",
    "        hash_count: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "    ) -> int:\n",
    "        if lowhash_fraction is None and lowhash_count is None:\n",
    "            raise TypeError(\n",
    "                \"Either `lowhash_fraction` or `lowhash_count` must be specified.\"\n",
    "            )\n",
    "        if lowhash_fraction is not None and lowhash_count is not None:\n",
    "            raise TypeError(\n",
    "                f\"`lowhash_fraction` and `lowhash_count` cannot be specified at the same time. {lowhash_fraction=} {lowhash_count=}\"\n",
    "            )\n",
    "\n",
    "        if lowhash_fraction is not None:\n",
    "            lowhash_count = ceil(hash_count * lowhash_fraction)\n",
    "            lowhash_count = max(lowhash_count, 1)\n",
    "        if lowhash_count is None:\n",
    "            raise ValueError()\n",
    "        return lowhash_count\n",
    "    \n",
    "    def _lowhash(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        repeats: int,\n",
    "        lowhash_fraction: float | None,\n",
    "        lowhash_count: int | None = None,\n",
    "        seed: int = 5731343,\n",
    "        verbose=True,\n",
    "    ) -> csr_matrix:\n",
    "        sample_count, feature_count = data.shape\n",
    "        buckets = sparse.dok_matrix(\n",
    "            (feature_count * repeats, sample_count), dtype=np.bool_\n",
    "        )\n",
    "\n",
    "        # Calculate hash values\n",
    "        hash_values = self._get_hash_values(\n",
    "            np.arange(feature_count), repeats=repeats, seed=seed\n",
    "        )\n",
    "\n",
    "        # For each sample, find the lowest hash values for its features\n",
    "        for j in range(sample_count):\n",
    "            feature_indices = sparse.find(data[j, :] > 0)[1]\n",
    "            hash_count = feature_indices.shape[0]\n",
    "            sample_lowhash_count = self._get_lowhash_count(\n",
    "                hash_count=hash_count,\n",
    "                lowhash_fraction=lowhash_fraction,\n",
    "                lowhash_count=lowhash_count,\n",
    "            )\n",
    "            for k in range(repeats):\n",
    "                bucket_indices = feature_indices + (k * feature_count)\n",
    "                sample_hash_values = hash_values[bucket_indices]\n",
    "                low_hash_buckets = bucket_indices[\n",
    "                    np.argsort(sample_hash_values)[:sample_lowhash_count]\n",
    "                ]\n",
    "                buckets[low_hash_buckets, j] = 1\n",
    "            if verbose and j % 1000 == 0:\n",
    "                print(j, end=\" \")\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        buckets = sparse.csr_matrix(buckets)\n",
    "        return buckets\n",
    "\n",
    "    def _get_adjacency_matrix(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        buckets: csr_matrix,\n",
    "        n_neighbors: int,\n",
    "        min_bucket_size,\n",
    "        max_bucket_size,\n",
    "        min_cooccurence_count,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        # Select neighbor candidates based on cooccurence counts\n",
    "        row_sums = buckets.sum(axis=1).A1  # type: ignore\n",
    "        matrix = buckets[\n",
    "            (row_sums >= min_bucket_size) & (row_sums <= max_bucket_size), :\n",
    "        ].astype(np.uint8)\n",
    "        cooccurrence_matrix = matrix.T.dot(matrix)\n",
    "\n",
    "        neighbor_dict = collections.defaultdict(dict)\n",
    "        nonzero_indices = list(zip(*cooccurrence_matrix.nonzero()))\n",
    "        for i, j in nonzero_indices:\n",
    "            if i >= j: \n",
    "                continue\n",
    "\n",
    "            count = cooccurrence_matrix[i, j]\n",
    "            neighbor_dict[i][j] = count\n",
    "            neighbor_dict[j][i] = count\n",
    "\n",
    "        # Construct neighbor matrix\n",
    "        n_rows = data.shape[0]\n",
    "        nbr_matrix = []\n",
    "        for i in range(n_rows)[1000:]:\n",
    "            row_nbr_dict = {\n",
    "                j: count\n",
    "                for j, count in neighbor_dict[i].items()\n",
    "                if count >= min_cooccurence_count and j < 1000\n",
    "            }\n",
    "            neighbors = list(\n",
    "                sorted(row_nbr_dict, key=lambda x: row_nbr_dict[x], reverse=True)\n",
    "            )[:n_neighbors]\n",
    "            nbr_matrix.append(neighbors)\n",
    "        return nbr_matrix\n",
    "\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "        repeats=100,\n",
    "        min_bucket_size=2,\n",
    "        max_bucket_size=float(\"inf\"),\n",
    "        min_cooccurence_count=1,\n",
    "        *,\n",
    "        seed=1,\n",
    "        verbose=True,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        buckets = self._lowhash(\n",
    "            data,\n",
    "            repeats=repeats,\n",
    "            lowhash_fraction=lowhash_fraction,\n",
    "            lowhash_count=lowhash_count,\n",
    "            seed=seed,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        nbr_matrix = self._get_adjacency_matrix(\n",
    "            data,\n",
    "            buckets,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_bucket_size=min_bucket_size,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            min_cooccurence_count=min_cooccurence_count,\n",
    "        )\n",
    "        return nbr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERAGE_DEPTH = 20\n",
    "max_bucket_size = COVERAGE_DEPTH * 1.5\n",
    "indices = LowHash().get_neighbors(data=merged_matrix,repeats=100,lowhash_count=20,n_neighbors=1,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            seed=458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3172702e82c9c1d9450fdd20452651b9_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
