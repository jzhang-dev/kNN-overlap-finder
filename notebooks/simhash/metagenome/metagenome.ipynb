{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from snakemake_stub import *\n",
    "\n",
    "\n",
    "import gzip, json, collections\n",
    "from typing import Sequence, Mapping, Collection\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "def init_reverse_complement():\n",
    "    TRANSLATION_TABLE = str.maketrans(\"ACTGactg\", \"TGACtgac\")\n",
    "\n",
    "    def reverse_complement(sequence: str) -> str:\n",
    "        \"\"\"\n",
    "        >>> reverse_complement(\"AATC\")\n",
    "        'GATT'\n",
    "        >>> reverse_complement(\"CCANT\")\n",
    "        'ANTGG'\n",
    "        \"\"\"\n",
    "        sequence = str(sequence)\n",
    "        return sequence.translate(TRANSLATION_TABLE)[::-1]\n",
    "\n",
    "    return reverse_complement\n",
    "\n",
    "\n",
    "reverse_complement = init_reverse_complement()\n",
    "import mmh3\n",
    "import sharedmem\n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "import numpy as np  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_database = '/home/miaocj/docker_dir/data/metagenome/part1.fa'\n",
    "query_reads = '/home/miaocj/docker_dir/data/metagenome/pbsim_ONT_95_30k_10dep_part1_reads.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_tax_list = []\n",
    "with open(ref_database) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            line = lines.strip().split(' ')\n",
    "            ref_reads_tax_list.append(line[1]+line[2])\n",
    "ref_read_tax = {i:tax for i,tax in enumerate(ref_reads_tax_list)}\n",
    "\n",
    "flag = 0\n",
    "que_read_tax = {}\n",
    "with open(query_reads) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            start = lines.index('S')\n",
    "            end = lines.index('_')\n",
    "            ref_num = lines[start+1:end]\n",
    "            que_read_tax[flag] = ref_read_tax[int(ref_num)-1]\n",
    "            flag +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reads(fasta_path: str):\n",
    "    read_sequences = []\n",
    "    read_names = []\n",
    "    read_orientations = []\n",
    "\n",
    "    with open(fasta_path, \"rt\") as handle:  # Open gzipped file in text mode\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            read_sequences.append(seq)\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"+\")\n",
    "\n",
    "            # Include reverse complement\n",
    "            read_sequences.append(reverse_complement(seq))\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"-\")\n",
    "\n",
    "    return read_names, read_orientations, read_sequences\n",
    "\n",
    "read_names, read_orientations, read_sequences = load_reads(ref_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 276256424, 2: 23715451, 3: 3702956, 4: 1039535, 5: 423488, 6: 221958, 7: 134826, 8: 86799, 9: 59680, 10: 48269})\n"
     ]
    }
   ],
   "source": [
    "def build_kmer_index(\n",
    "    read_sequences: Sequence[str],\n",
    "    k: int,\n",
    "    *,\n",
    "    sample_fraction: float,\n",
    "    min_multiplicity: int,\n",
    "    seed: int,\n",
    ") -> Mapping[str, int]:\n",
    "    kmer_counter = collections.Counter()\n",
    "    for seq in read_sequences:\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            kmer_counter[kmer] += 1\n",
    "\n",
    "    kmer_spectrum = collections.Counter(x for x in kmer_counter.values() if x <= 10)\n",
    "    print(kmer_spectrum)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    vocabulary = set(\n",
    "        x\n",
    "        for x, count in kmer_counter.items()\n",
    "        if count >= min_multiplicity and rng.random() <= sample_fraction\n",
    "    )\n",
    "    vocabulary |= set(reverse_complement(x) for x in vocabulary)\n",
    "    kmer_indices = {kmer: i for i, kmer in enumerate(vocabulary)}\n",
    "    return kmer_indices\n",
    "\n",
    "sample_fraction=0.1\n",
    "min_multiplicity=2\n",
    "seed=562104830\n",
    "kmer_indices = build_kmer_index(        \n",
    "        read_sequences=read_sequences,\n",
    "        k=16,\n",
    "        sample_fraction=sample_fraction,\n",
    "        min_multiplicity=min_multiplicity,\n",
    "        seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qread_names, qread_orientations, qread_sequences = load_reads(query_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(\n",
    "    read_sequences: Sequence[str],\n",
    "    kmer_indices: Mapping[str, int],\n",
    "    k: int,\n",
    ") -> tuple[sp.csr_matrix, Sequence[Sequence[int]]]:\n",
    "    row_ind, col_ind, data = [], [], []\n",
    "    read_features = []\n",
    "    for i, seq in enumerate(read_sequences):\n",
    "        features_i = []\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            j = kmer_indices.get(kmer)\n",
    "            if j is None:\n",
    "                continue\n",
    "            features_i.append(j)\n",
    "\n",
    "        read_features.append(features_i)\n",
    "\n",
    "        kmer_counts = collections.Counter(features_i)\n",
    "        for j, count in kmer_counts.items():\n",
    "            row_ind.append(i)\n",
    "            col_ind.append(j)\n",
    "            data.append(count)\n",
    "\n",
    "    feature_matrix = sp.csr_matrix(\n",
    "        (data, (row_ind, col_ind)), shape=(len(read_sequences), len(kmer_indices))\n",
    "    )\n",
    "    return feature_matrix, read_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate sensitivity and precision\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score  \n",
    "\n",
    "def evaluate(indices):\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    for query_read_num,x in enumerate(indices):\n",
    "        neighbor = x[0]\n",
    "        neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "        query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "        prediction.append(ref_read_tax[neighbor])\n",
    "        actual.append(que_read_tax[query_read_num])\n",
    "\n",
    "    precision = precision_score(actual, prediction,average='macro')\n",
    "    sensitivity = recall_score(actual, prediction,average='macro')\n",
    "    ##计算每个类别的\n",
    "    precision_sep = precision_score(actual, prediction, average=None)  \n",
    "    sensitivity_sep = recall_score(actual, prediction, average=None)\n",
    "    return precision,sensitivity,precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feature_matrix,ref_read_features = build_feature_matrix(read_sequences=read_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5647520)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isal import igzip\n",
    "def open_gzipped(path, mode=\"rt\", gzipped: bool | None = None, **kw):\n",
    "    if gzipped is None:\n",
    "        gzipped = path.endswith(\".gz\")\n",
    "    if gzipped:\n",
    "        open_ = igzip.open\n",
    "        return open_(path, mode)\n",
    "    else:\n",
    "        open_ = open\n",
    "    return open_(path, mode, **kw)\n",
    "                   \n",
    "output_npz_file = '/home/miaocj/docker_dir/data/metagenome/ref_feature_matrix.npz'\n",
    "output_json_file = '/home/miaocj/docker_dir/data/metagenome/ref_read_features.json.gz'\n",
    "sp.save_npz(output_npz_file, ref_feature_matrix)\n",
    "with open_gzipped(output_json_file, \"wt\") as f:\n",
    "    json.dump(ref_read_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_feature_matrix,que_read_features = build_feature_matrix(read_sequences=qread_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##存储\n",
    "output_npz_file2 = '/home/miaocj/docker_dir/data/metagenome/que_feature_matrix.npz'\n",
    "output_json_file2 = '/home/miaocj/docker_dir/data/metagenome/que_read_features.json.gz'\n",
    "sp.save_npz(output_npz_file2, que_feature_matrix)\n",
    "with open_gzipped(output_json_file2, \"wt\") as f:\n",
    "    json.dump(que_read_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读取\n",
    "output_npz_file2 = '/home/miaocj/docker_dir/data/metagenome/que_feature_matrix.npz'\n",
    "output_json_file2 = '/home/miaocj/docker_dir/data/metagenome/que_read_features.json.gz'\n",
    "with gzip.open(output_json_file2, \"rt\") as f:\n",
    "    que_read_features = json.load(f)\n",
    "que_feature_matrix = sp.load_npz(output_npz_file2)\n",
    "\n",
    "output_npz_file = '/home/miaocj/docker_dir/data/metagenome/ref_feature_matrix.npz'\n",
    "output_json_file = '/home/miaocj/docker_dir/data/metagenome/ref_read_features.json.gz'\n",
    "with gzip.open(output_json_file, \"rt\") as f:\n",
    "    ref_read_features = json.load(f)\n",
    "ref_feature_matrix = sp.load_npz(output_npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119250, 5647520)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_indices = [index for index, sublist in enumerate(que_read_features) if len(sublist) == 0]  \n",
    "empty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "merged_matrix = sp.vstack([ref_feature_matrix, que_feature_matrix])  \n",
    "from dim_reduction import scBiMapEmbedding\n",
    "dim500 = scBiMapEmbedding().transform(merged_matrix,n_dimensions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim500_ref = dim500[:1000]\n",
    "dim500_que = dim500[1001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 降维+精确求解\n",
    "\n",
    "query_vectors = np.array(dim500_que) \n",
    "database_vectors = np.array(dim500_ref) \n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto',metric='cosine').fit(database_vectors)  \n",
    "distances, indices = nbrs.kneighbors(query_vectors)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.970391944806762, 0.9575761776138907)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categories = []\n",
    "for i in range(1,13):\n",
    "    categories.append(i)\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=300) \n",
    "plt.bar(categories,precision_sep,color = 'gray')\n",
    "\n",
    "plt.ylabel('Genus Precision')\n",
    "plt.ylim(0.6,1.01)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.gca().spines['right'].set_color('none')  \n",
    "plt.gca().spines['top'].set_color('none') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "plt.figure(figsize=(6, 6), dpi=300) \n",
    "plt.scatter(precision_sep,sensitivity_sep, label='Data points', marker='o', c=sensitivity_sep, cmap='rainbow')\n",
    "\n",
    "plt.xlim(0.8,1.01)\n",
    "plt.ylim(0.8,1.01)\n",
    "plt.gca().spines['right'].set_color('none')  \n",
    "plt.gca().spines['top'].set_color('none') \n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.margins(0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9584399030599837\n"
     ]
    }
   ],
   "source": [
    "##rough evaluate\n",
    "right_num = 0\n",
    "for query_read_num,x in enumerate(indices):\n",
    "    neighbor = x[0]\n",
    "    neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "    query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "    if ref_read_tax[neighbor] == que_read_tax[query_read_num]:\n",
    "        right_num +=1\n",
    "\n",
    "print(right_num/len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87046535, 0.62569832, 0.83373867, 0.86720252, 0.89721499,\n",
       "        0.87860642, 0.86374657, 0.41751232, 0.90688083, 0.54479376,\n",
       "        0.83079625, 0.93578767]),\n",
       " array([0.47510944, 0.98778833, 0.61177807, 0.68116937, 0.66164303,\n",
       "        0.6721266 , 0.60990641, 0.99246462, 0.41897312, 0.68407741,\n",
       "        0.71957404, 0.33051104]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "\n",
    "class HNSW():\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        ref_data: csr_matrix | np.ndarray,\n",
    "        que_data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        metric: Literal[\"euclidean\", \"cosine\"] = \"euclidean\",\n",
    "        *,\n",
    "        threads: int | None = None,\n",
    "        M: int = 16,\n",
    "        ef_construction: int = 200,\n",
    "        ef_search: int = 50,\n",
    "    ) -> np.ndarray:\n",
    "        if metric == \"euclidean\":\n",
    "            space = \"l2\"\n",
    "        else:\n",
    "            space = metric\n",
    "\n",
    "        # Initialize the HNSW index\n",
    "        p = hnswlib.Index(space=space, dim=ref_data.shape[1])\n",
    "        if threads is not None:\n",
    "            p.set_num_threads(threads)\n",
    "        p.init_index(max_elements=ref_data.shape[0], ef_construction=ef_construction, M=M)\n",
    "        ids = np.arange(ref_data.shape[0])\n",
    "        p.add_items(ref_data, ids)\n",
    "        p.set_ef(ef_search)\n",
    "        nbr_indices, _ = p.knn_query(que_data, k=n_neighbors)\n",
    "        return nbr_indices\n",
    "nbr_indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dim500_HNSW\n",
    "\n",
    "indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9581952089324665, 0.952445601680645)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "import collections\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy import sparse\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import mmh3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LowHash():\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash(x: int, seed: int) -> int:\n",
    "        hash_value = mmh3.hash(str(x), seed=seed)\n",
    "        return hash_value\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hash_values(data: Iterable[int], repeats: int, seed: int) -> np.ndarray:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeats, dtype=np.uint64)\n",
    "        hash_values = []\n",
    "        for k in range(repeats):\n",
    "            s = hash_seeds[k]\n",
    "            for x in data:\n",
    "                hash_values.append(LowHash._hash(x, seed=s))\n",
    "        hash_values = np.array(hash_values, dtype=np.int64)\n",
    "        return hash_values\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lowhash_count(\n",
    "        hash_count: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "    ) -> int:\n",
    "        if lowhash_fraction is None and lowhash_count is None:\n",
    "            raise TypeError(\n",
    "                \"Either `lowhash_fraction` or `lowhash_count` must be specified.\"\n",
    "            )\n",
    "        if lowhash_fraction is not None and lowhash_count is not None:\n",
    "            raise TypeError(\n",
    "                f\"`lowhash_fraction` and `lowhash_count` cannot be specified at the same time. {lowhash_fraction=} {lowhash_count=}\"\n",
    "            )\n",
    "\n",
    "        if lowhash_fraction is not None:\n",
    "            lowhash_count = ceil(hash_count * lowhash_fraction)\n",
    "            lowhash_count = max(lowhash_count, 1)\n",
    "        if lowhash_count is None:\n",
    "            raise ValueError()\n",
    "        return lowhash_count\n",
    "    \n",
    "    def _lowhash(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        repeats: int,\n",
    "        lowhash_fraction: float | None,\n",
    "        lowhash_count: int | None = None,\n",
    "        seed: int = 5731343,\n",
    "        verbose=True,\n",
    "    ) -> csr_matrix:\n",
    "        sample_count, feature_count = data.shape\n",
    "        buckets = sparse.dok_matrix(\n",
    "            (feature_count * repeats, sample_count), dtype=np.bool_\n",
    "        )\n",
    "\n",
    "        # Calculate hash values\n",
    "        hash_values = self._get_hash_values(\n",
    "            np.arange(feature_count), repeats=repeats, seed=seed\n",
    "        )\n",
    "\n",
    "        # For each sample, find the lowest hash values for its features\n",
    "        for j in range(sample_count):\n",
    "            feature_indices = sparse.find(data[j, :] > 0)[1]\n",
    "            hash_count = feature_indices.shape[0]\n",
    "            sample_lowhash_count = self._get_lowhash_count(\n",
    "                hash_count=hash_count,\n",
    "                lowhash_fraction=lowhash_fraction,\n",
    "                lowhash_count=lowhash_count,\n",
    "            )\n",
    "            for k in range(repeats):\n",
    "                bucket_indices = feature_indices + (k * feature_count)\n",
    "                sample_hash_values = hash_values[bucket_indices]\n",
    "                low_hash_buckets = bucket_indices[\n",
    "                    np.argsort(sample_hash_values)[:sample_lowhash_count]\n",
    "                ]\n",
    "                buckets[low_hash_buckets, j] = 1\n",
    "            if verbose and j % 1000 == 0:\n",
    "                print(j, end=\" \")\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        buckets = sparse.csr_matrix(buckets)\n",
    "        return buckets\n",
    "\n",
    "    def _get_adjacency_matrix(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        buckets: csr_matrix,\n",
    "        n_neighbors: int,\n",
    "        min_bucket_size,\n",
    "        max_bucket_size,\n",
    "        min_cooccurence_count,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        # Select neighbor candidates based on cooccurence counts\n",
    "        row_sums = buckets.sum(axis=1).A1  # type: ignore\n",
    "        matrix = buckets[\n",
    "            (row_sums >= min_bucket_size) & (row_sums <= max_bucket_size), :\n",
    "        ].astype(np.uint8)\n",
    "        cooccurrence_matrix = matrix.T.dot(matrix)\n",
    "\n",
    "        neighbor_dict = collections.defaultdict(dict)\n",
    "        nonzero_indices = list(zip(*cooccurrence_matrix.nonzero()))\n",
    "        for i, j in nonzero_indices:\n",
    "            if i >= j: \n",
    "                continue\n",
    "\n",
    "            count = cooccurrence_matrix[i, j]\n",
    "            neighbor_dict[i][j] = count\n",
    "            neighbor_dict[j][i] = count\n",
    "\n",
    "        # Construct neighbor matrix\n",
    "        n_rows = data.shape[0]\n",
    "        nbr_matrix = []\n",
    "        for i in range(n_rows)[1000:]:\n",
    "            row_nbr_dict = {\n",
    "                j: count\n",
    "                for j, count in neighbor_dict[i].items()\n",
    "                if count >= min_cooccurence_count and j < 1000\n",
    "            }\n",
    "            neighbors = list(\n",
    "                sorted(row_nbr_dict, key=lambda x: row_nbr_dict[x], reverse=True)\n",
    "            )[:n_neighbors]\n",
    "            nbr_matrix.append(neighbors)\n",
    "        return nbr_matrix\n",
    "\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "        repeats=100,\n",
    "        min_bucket_size=2,\n",
    "        max_bucket_size=float(\"inf\"),\n",
    "        min_cooccurence_count=1,\n",
    "        *,\n",
    "        seed=1,\n",
    "        verbose=True,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        buckets = self._lowhash(\n",
    "            data,\n",
    "            repeats=repeats,\n",
    "            lowhash_fraction=lowhash_fraction,\n",
    "            lowhash_count=lowhash_count,\n",
    "            seed=seed,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        nbr_matrix = self._get_adjacency_matrix(\n",
    "            data,\n",
    "            buckets,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_bucket_size=min_bucket_size,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            min_cooccurence_count=min_cooccurence_count,\n",
    "        )\n",
    "        return nbr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERAGE_DEPTH = 20\n",
    "max_bucket_size = COVERAGE_DEPTH * 1.5\n",
    "indices = LowHash().get_neighbors(data=merged_matrix,repeats=100,lowhash_count=20,n_neighbors=1,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            seed=458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
