{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from snakemake_stub import *\n",
    "\n",
    "\n",
    "import gzip, json, collections\n",
    "from typing import Sequence, Mapping, Collection\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4262b1bf4bf1ffb403c0eb7a42ad5906_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/4506eccf78279d93d0e8a34c035e91c5_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/6bda807e3967eae797c7b1b9eeaee8db_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c2a47d89d1d34e789fdf782557bb7194_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/c6c5514ada15b890fb27d1e36371554c_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/d964a294c2d0fef56a434c021026281e_/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"/home/miaocj/docker_dir/kNN-overlap-finder/.snakemake/conda/e1c932db5cd4271709e54d8028824bc9_/lib/python3.12/site-packages\")\n",
    "def init_reverse_complement():\n",
    "    TRANSLATION_TABLE = str.maketrans(\"ACTGactg\", \"TGACtgac\")\n",
    "\n",
    "    def reverse_complement(sequence: str) -> str:\n",
    "        \"\"\"\n",
    "        >>> reverse_complement(\"AATC\")\n",
    "        'GATT'\n",
    "        >>> reverse_complement(\"CCANT\")\n",
    "        'ANTGG'\n",
    "        \"\"\"\n",
    "        sequence = str(sequence)\n",
    "        return sequence.translate(TRANSLATION_TABLE)[::-1]\n",
    "\n",
    "    return reverse_complement\n",
    "\n",
    "\n",
    "reverse_complement = init_reverse_complement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_database = '/home/miaocj/docker_dir/data/metagenome/part1/ref2.fa'\n",
    "query_reads = '/home/miaocj/docker_dir/data/metagenome/part1/diff_depth2.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agaricusbisporus',\n",
       " 'Eremotheciumcymbalariae',\n",
       " 'Kazachstaniaafricana',\n",
       " 'Phaeoacremoniumminimum']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_reads_tax_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reads(fasta_path: str):\n",
    "    read_sequences = []\n",
    "    read_names = []\n",
    "    read_orientations = []\n",
    "    read_length = []\n",
    "\n",
    "    with open(fasta_path, \"rt\") as handle:  # Open gzipped file in text mode\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            read_sequences.append(seq)\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"+\")\n",
    "            read_length.append(len(record))\n",
    "\n",
    "            # Include reverse complement\n",
    "            read_sequences.append(reverse_complement(seq))\n",
    "            read_names.append(record.id)\n",
    "            read_orientations.append(\"-\")\n",
    "            read_length.append(len(record))\n",
    "\n",
    "    return read_names, read_orientations, read_sequences,read_length\n",
    "\n",
    "read_names, read_orientations, read_sequences, read_length = load_reads(ref_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 6853434, 2: 90134, 3: 25438, 4: 10907, 5: 6236, 6: 3622, 7: 2564, 8: 1742, 9: 1320, 10: 810})\n"
     ]
    }
   ],
   "source": [
    "def build_kmer_index(\n",
    "    read_sequences: Sequence[str],\n",
    "    k: int,\n",
    "    *,\n",
    "    sample_fraction: float,\n",
    "    min_multiplicity: int,\n",
    "    seed: int,\n",
    ") -> Mapping[str, int]:\n",
    "    kmer_counter = collections.Counter()\n",
    "    for seq in read_sequences:\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            kmer_counter[kmer] += 1\n",
    "\n",
    "    kmer_spectrum = collections.Counter(x for x in kmer_counter.values() if x <= 10)\n",
    "    print(kmer_spectrum)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    vocabulary = set(\n",
    "        x\n",
    "        for x, count in kmer_counter.items()\n",
    "        if count >= min_multiplicity and rng.random() <= sample_fraction\n",
    "    )\n",
    "    vocabulary |= set(reverse_complement(x) for x in vocabulary)\n",
    "    kmer_indices = {kmer: i for i, kmer in enumerate(vocabulary)}\n",
    "    return kmer_indices\n",
    "\n",
    "sample_fraction=0.05\n",
    "min_multiplicity=2\n",
    "seed=562104830\n",
    "kmer_indices = build_kmer_index(        \n",
    "        read_sequences=read_sequences,\n",
    "        k=16,\n",
    "        sample_fraction=sample_fraction,\n",
    "        min_multiplicity=min_multiplicity,\n",
    "        seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "qread_names, qread_orientations, qread_sequence, qread_length  = load_reads(query_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_reads_tax_list = []\n",
    "with open(ref_database) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            line = lines.strip().split(' ')\n",
    "            ref_reads_tax_list.append(line[1]+line[2])\n",
    "ref_read_tax = {i:tax for i,tax in enumerate(ref_reads_tax_list)}\n",
    "ref_length = {i:read_length[i] for i,tax in enumerate(ref_reads_tax_list)}\n",
    "\n",
    "flag = 0\n",
    "que_read_tax = {}\n",
    "indice_length = {}\n",
    "with open(query_reads) as file:\n",
    "    for lines in file:\n",
    "        if lines[0] == '>':\n",
    "            end = lines.index('_')\n",
    "            ref_num = lines[1:end]\n",
    "            que_read_tax[flag] = ref_num\n",
    "            indice_length[flag] = qread_length[flag]\n",
    "            flag +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(\n",
    "    read_sequences: Sequence[str],\n",
    "    kmer_indices: Mapping[str, int],\n",
    "    k: int,\n",
    ") -> tuple[sp.csr_matrix, Sequence[Sequence[int]]]:\n",
    "    row_ind, col_ind, data = [], [], []\n",
    "    read_features = []\n",
    "    for i, seq in enumerate(read_sequences):\n",
    "        features_i = []\n",
    "        for p in range(len(seq) - k + 1):\n",
    "            kmer = seq[p : p + k]\n",
    "            j = kmer_indices.get(kmer)\n",
    "            if j is None:\n",
    "                continue\n",
    "            features_i.append(j)\n",
    "\n",
    "        read_features.append(features_i)\n",
    "\n",
    "        kmer_counts = collections.Counter(features_i)\n",
    "        for j, count in kmer_counts.items():\n",
    "            row_ind.append(i)\n",
    "            col_ind.append(j)\n",
    "            data.append(count)\n",
    "\n",
    "    feature_matrix = sp.csr_matrix(\n",
    "        (data, (row_ind, col_ind)), shape=(len(read_sequences), len(kmer_indices))\n",
    "    )\n",
    "    return feature_matrix, read_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate sensitivity and precision\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score  \n",
    "\n",
    "def evaluate(indices):\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    for query_read_num,x in enumerate(indices):\n",
    "        neighbor = x[0]\n",
    "        neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "        query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "        prediction.append(ref_read_tax[neighbor])\n",
    "        actual.append(que_read_tax[query_read_num])\n",
    "\n",
    "    precision = precision_score(actual, prediction,average='macro')\n",
    "    sensitivity = recall_score(actual, prediction,average='macro')\n",
    "    ##计算每个类别的\n",
    "    precision_sep = precision_score(actual, prediction, average=None)  \n",
    "    sensitivity_sep = recall_score(actual, prediction, average=None)\n",
    "    return precision,sensitivity,precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feature_matrix,ref_read_features = build_feature_matrix(read_sequences=read_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 14078)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_feature_matrix,que_read_features = build_feature_matrix(read_sequences=qread_sequences,kmer_indices=kmer_indices,k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4302, 14078)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "merged_matrix = sp.vstack([ref_feature_matrix, que_feature_matrix])  \n",
    "from dim_reduction import scBiMapEmbedding\n",
    "dim500 = scBiMapEmbedding().transform(merged_matrix,n_dimensions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim500_ref = dim500[:8]\n",
    "dim500_que = dim500[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 降维+精确求解\n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "import numpy as np  \n",
    "\n",
    "query_vectors = np.array(dim500_que) \n",
    "database_vectors = np.array(dim500_ref) \n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto',metric='cosine').fit(database_vectors)  \n",
    "distances, indices_exact = nbrs.kneighbors(query_vectors)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices_exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "prediction = []\n",
    "for query_read_num,x in enumerate(indices_exact):\n",
    "    neighbor = x[0]\n",
    "    neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "    query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "    prediction.append(ref_read_tax[neighbor])\n",
    "    actual.append(que_read_tax[query_read_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agaricusbisporus\n",
      "17.496957207641202\n",
      "Eremotheciumcymbalariae\n",
      "22.542886990411617\n",
      "Kazachstaniaafricana\n",
      "23.512340437859905\n",
      "Phaeoacremoniumminimum\n",
      "45.30084371824357\n"
     ]
    }
   ],
   "source": [
    "for i,tax in ref_read_tax.items():\n",
    "    pre_indices = [index for index, value in enumerate(prediction) if value == tax]\n",
    "    pre_len = sum([qread_length[x] for x in pre_indices])\n",
    "    actual_len = read_length[i]\n",
    "    pred_depth = pre_len/actual_len\n",
    "    print(tax)\n",
    "    print(pred_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1748887, 1: 897456, 2: 421466, 3: 592725}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22524752475247525, 91, 313, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 'Encephalitozoonromaleae'\n",
    "true_positive = sum([1 for p, a in zip(prediction, actual) if p == target_class and a == target_class])\n",
    "false_positive = sum([1 for p, a in zip(prediction, actual) if p == target_class and a != target_class])\n",
    "false_negative = sum([1 for p, a in zip(prediction, actual) if p != target_class and a == target_class])\n",
    "\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "precision,true_positive,false_positive,false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "plt.figure(figsize=(6, 6), dpi=300) \n",
    "plt.scatter(precision_sep,sensitivity_sep, label='Data points', marker='o', c=sensitivity_sep, cmap='rainbow')\n",
    "\n",
    "plt.gca().spines['right'].set_color('none')  \n",
    "plt.gca().spines['top'].set_color('none') \n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.margins(0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9584399030599837\n"
     ]
    }
   ],
   "source": [
    "##rough evaluate\n",
    "right_num = 0\n",
    "for query_read_num,x in enumerate(indices):\n",
    "    neighbor = x[0]\n",
    "    neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "    query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "    if ref_read_tax[neighbor] == que_read_tax[query_read_num]:\n",
    "        right_num +=1\n",
    "\n",
    "print(right_num/len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "\n",
    "class HNSW():\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        ref_data: csr_matrix | np.ndarray,\n",
    "        que_data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        metric: Literal[\"euclidean\", \"cosine\"] = \"euclidean\",\n",
    "        *,\n",
    "        threads: int | None = None,\n",
    "        M: int = 16,\n",
    "        ef_construction: int = 200,\n",
    "        ef_search: int = 50,\n",
    "    ) -> np.ndarray:\n",
    "        if metric == \"euclidean\":\n",
    "            space = \"l2\"\n",
    "        else:\n",
    "            space = metric\n",
    "\n",
    "        # Initialize the HNSW index\n",
    "        p = hnswlib.Index(space=space, dim=ref_data.shape[1])\n",
    "        if threads is not None:\n",
    "            p.set_num_threads(threads)\n",
    "        p.init_index(max_elements=ref_data.shape[0], ef_construction=ef_construction, M=M)\n",
    "        ids = np.arange(ref_data.shape[0])\n",
    "        p.add_items(ref_data, ids)\n",
    "        p.set_ef(ef_search)\n",
    "        nbr_indices, _ = p.knn_query(que_data, k=n_neighbors)\n",
    "        return nbr_indices\n",
    "nbr_indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dim500_HNSW\n",
    "\n",
    "indices = HNSW().get_neighbors(ref_data=dim500_ref,que_data=dim500_que,n_neighbors=1)\n",
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.94196891, 0.22524752, 0.9897541 , 0.81634183]),\n",
       " array([0.8015873 , 0.98913043, 0.57913669, 0.96286472]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import lru_cache\n",
    "import collections\n",
    "from typing import Sequence, Type, Mapping, Iterable, Literal\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "from scipy import sparse\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import numpy as np\n",
    "import mmh3\n",
    "\n",
    "\n",
    "\n",
    "class LowHash():\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash(x: int, seed: int) -> int:\n",
    "        hash_value = mmh3.hash(str(x), seed=seed)\n",
    "        return hash_value\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hash_values(data: Iterable[int], repeats: int, seed: int) -> np.ndarray:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        hash_seeds = rng.integers(low=0, high=2**32 - 1, size=repeats, dtype=np.uint64)\n",
    "        hash_values = []\n",
    "        for k in range(repeats):\n",
    "            s = hash_seeds[k]\n",
    "            for x in data:\n",
    "                hash_values.append(LowHash._hash(x, seed=s))\n",
    "        hash_values = np.array(hash_values, dtype=np.int64)\n",
    "        return hash_values\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lowhash_count(\n",
    "        hash_count: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "    ) -> int:\n",
    "        if lowhash_fraction is None and lowhash_count is None:\n",
    "            raise TypeError(\n",
    "                \"Either `lowhash_fraction` or `lowhash_count` must be specified.\"\n",
    "            )\n",
    "        if lowhash_fraction is not None and lowhash_count is not None:\n",
    "            raise TypeError(\n",
    "                f\"`lowhash_fraction` and `lowhash_count` cannot be specified at the same time. {lowhash_fraction=} {lowhash_count=}\"\n",
    "            )\n",
    "\n",
    "        if lowhash_fraction is not None:\n",
    "            lowhash_count = ceil(hash_count * lowhash_fraction)\n",
    "            lowhash_count = max(lowhash_count, 1)\n",
    "        if lowhash_count is None:\n",
    "            raise ValueError()\n",
    "        return lowhash_count\n",
    "    \n",
    "    def _lowhash(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        repeats: int,\n",
    "        lowhash_fraction: float | None,\n",
    "        lowhash_count: int | None = None,\n",
    "        seed: int = 5731343,\n",
    "        verbose=True,\n",
    "    ) -> csr_matrix:\n",
    "        sample_count, feature_count = data.shape\n",
    "        buckets = sparse.dok_matrix(\n",
    "            (feature_count * repeats, sample_count), dtype=np.bool_\n",
    "        )\n",
    "\n",
    "        # Calculate hash values\n",
    "        hash_values = self._get_hash_values(\n",
    "            np.arange(feature_count), repeats=repeats, seed=seed\n",
    "        )\n",
    "\n",
    "        # For each sample, find the lowest hash values for its features\n",
    "        for j in range(sample_count):\n",
    "            feature_indices = sparse.find(data[j, :] > 0)[1]\n",
    "            hash_count = feature_indices.shape[0]\n",
    "            sample_lowhash_count = self._get_lowhash_count(\n",
    "                hash_count=hash_count,\n",
    "                lowhash_fraction=lowhash_fraction,\n",
    "                lowhash_count=lowhash_count,\n",
    "            )\n",
    "            for k in range(repeats):\n",
    "                bucket_indices = feature_indices + (k * feature_count)\n",
    "                sample_hash_values = hash_values[bucket_indices]\n",
    "                low_hash_buckets = bucket_indices[\n",
    "                    np.argsort(sample_hash_values)[:sample_lowhash_count]\n",
    "                ]\n",
    "                buckets[low_hash_buckets, j] = 1\n",
    "            if verbose and j % 1000 == 0:\n",
    "                print(j, end=\" \")\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        buckets = sparse.csr_matrix(buckets)\n",
    "        return buckets\n",
    "\n",
    "    def _get_adjacency_matrix(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        buckets: csr_matrix,\n",
    "        n_neighbors: int,\n",
    "        min_bucket_size,\n",
    "        max_bucket_size,\n",
    "        min_cooccurence_count,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        # Select neighbor candidates based on cooccurence counts\n",
    "        row_sums = buckets.sum(axis=1).A1  # type: ignore\n",
    "        matrix = buckets[\n",
    "            (row_sums >= min_bucket_size) & (row_sums <= max_bucket_size), :\n",
    "        ].astype(np.uint8)\n",
    "        cooccurrence_matrix = matrix.T.dot(matrix)\n",
    "\n",
    "        neighbor_dict = collections.defaultdict(dict)\n",
    "        nonzero_indices = list(zip(*cooccurrence_matrix.nonzero()))\n",
    "        for i, j in nonzero_indices:\n",
    "            if i >= j: \n",
    "                continue\n",
    "\n",
    "            count = cooccurrence_matrix[i, j]\n",
    "            neighbor_dict[i][j] = count\n",
    "            neighbor_dict[j][i] = count\n",
    "\n",
    "        # Construct neighbor matrix\n",
    "        n_rows = data.shape[0]\n",
    "        nbr_matrix = []\n",
    "        for i in range(n_rows)[8:]:\n",
    "            row_nbr_dict = {\n",
    "                j: count\n",
    "                for j, count in neighbor_dict[i].items()\n",
    "                if count >= min_cooccurence_count and j < 8\n",
    "            }\n",
    "            neighbors = list(\n",
    "                sorted(row_nbr_dict, key=lambda x: row_nbr_dict[x], reverse=True)\n",
    "            )[:n_neighbors]\n",
    "            nbr_matrix.append(neighbors)\n",
    "        return nbr_matrix\n",
    "\n",
    "    def get_neighbors(\n",
    "        self,\n",
    "        data: csr_matrix | np.ndarray,\n",
    "        n_neighbors: int,\n",
    "        lowhash_fraction: float | None = None,\n",
    "        lowhash_count: int | None = None,\n",
    "        repeats=100,\n",
    "        min_bucket_size=2,\n",
    "        max_bucket_size=float(\"inf\"),\n",
    "        min_cooccurence_count=1,\n",
    "        *,\n",
    "        seed=1,\n",
    "        verbose=True,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        buckets = self._lowhash(\n",
    "            data,\n",
    "            repeats=repeats,\n",
    "            lowhash_fraction=lowhash_fraction,\n",
    "            lowhash_count=lowhash_count,\n",
    "            seed=seed,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        nbr_matrix = self._get_adjacency_matrix(\n",
    "            data,\n",
    "            buckets,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_bucket_size=min_bucket_size,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            min_cooccurence_count=min_cooccurence_count,\n",
    "        )\n",
    "        return nbr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 \n"
     ]
    }
   ],
   "source": [
    "COVERAGE_DEPTH = 20\n",
    "max_bucket_size = COVERAGE_DEPTH * 1.5\n",
    "indices = LowHash().get_neighbors(data=merged_matrix,repeats=100,lowhash_count=20,n_neighbors=1,\n",
    "            max_bucket_size=max_bucket_size,\n",
    "            seed=458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate sensitivity and precision\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score  \n",
    "\n",
    "def evaluate(indices):\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    for query_read_num,x in enumerate(indices):\n",
    "        if len (x) > 0:\n",
    "            neighbor = x[0]\n",
    "            neighbor = (neighbor-1)/2  if neighbor %2 !=0 else neighbor/2\n",
    "            query_read_num = (query_read_num-1)/2  if query_read_num %2 !=0 else query_read_num/2\n",
    "            prediction.append(ref_read_tax[neighbor])\n",
    "            actual.append(que_read_tax[query_read_num])\n",
    "\n",
    "    precision = precision_score(actual, prediction,average='macro')\n",
    "    sensitivity = recall_score(actual, prediction,average='macro')\n",
    "    ##计算每个类别的\n",
    "    precision_sep = precision_score(actual, prediction, average=None)  \n",
    "    sensitivity_sep = recall_score(actual, prediction, average=None)\n",
    "    return precision,sensitivity,precision_sep,sensitivity_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3192"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "empty_count = sum([1 for sublist in indices if len(sublist) == 0])\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision,sensitivity,precision_sep,sensitivity_sep = evaluate(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98910082, 0.23989218, 0.90131579, 0.89467593]),\n",
       " array([0.68815166, 1.        , 0.83664122, 0.99357326]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_sep,sensitivity_sep\n",
    "##输入种类比较少的时候，一些本身reads很少的genome 可以设置一个阈值，超过阈值的才被判断为近邻/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
